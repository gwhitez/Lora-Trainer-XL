{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gwhitez/Lora-Trainer-XL/blob/main/Old_Fix_Lora_Trainer_XL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omCLgUOB2_ZF"
      },
      "source": [
        "# üåü XL Lora Trainer\n",
        "\n",
        "‚ùó **Se recomienda Colab Premium.** Lo ideal ser√≠a cambiar el tiempo de ejecuci√≥n a una A100 y utilizar el tama√±o m√°ximo de lote.  \n",
        "Sin embargo, a√∫n se puede entrenar de forma gratuita si carga un modelo de diffusers, solo que tomar√° mucho m√°s tiempo.  \n",
        "\n",
        "\n",
        "This colab is based on the work of [Kohya-ss](https://github.com/kohya-ss/sd-scripts) and [Linaqruf](https://github.com/Linaqruf/kohya-trainer). Thank you!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJ8clWTZEu-g"
      },
      "source": [
        "### ‚≠ï Disclaimer\n",
        "The purpose of this document is to research bleeding-edge technologies in the field of machine learning inference.  \n",
        "Please read and follow the [Google Colab guidelines](https://research.google.com/colaboratory/faq.html) and its [Terms of Service](https://research.google.com/colaboratory/tos_v3.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPQlB4djNm3C"
      },
      "source": [
        "| |GitHub|Trainer XL|Tagger WDV2|Tagger WDV3||Old Trainer XL|\n",
        "|:--|:-:|:-:|:-:||:-:|:-:|\n",
        "| üè† **Original Proyect** |[![GitHub](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/github.svg)](https://github.com/hollowstrawberry/kohya-colab) | |\n",
        "|**Modified By WhiteZ**| [![GitHub](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/github.svg)](https://github.com/gwhitez/Lora-Trainer-XL) |   [![Open in Colab](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/colab-badge.svg)](https://colab.research.google.com/github/gwhitez/Lora-Trainer-XL/blob/main/Fix_Lora_Trainer_XL.ipynb) |[![Open in Colab](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/colab-badge.svg)](https://colab.research.google.com/github/gwhitez/Lora-Trainer-XL/blob/main/Dataset_Maker_By_WhiteZ.ipynb) | [![Open in Colab](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/colab-badge.svg)](https://colab.research.google.com/github/gwhitez/Lora-Trainer-XL/blob/main/Waifu_Diffusion_V3_Dataser_Maker.ipynb) |  |[![Open in Colab](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/colab-badge.svg)](https://colab.research.google.com/github/gwhitez/Lora-Trainer-XL/blob/main/Old_Fix_Lora_Trainer_XL.ipynb)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "AT1WRbhiHRCJ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import toml\n",
        "import pathlib\n",
        "import threading\n",
        "from time import time\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "#@markdown ### üí§ Auto Off\n",
        "\n",
        "#@markdown *Si esta opci√≥n est√° marcada, el entorno se desconectar√° autom√°ticamente una vez finalice el entrenamiento.*\n",
        "auto_off   = True  # @param {type: \"boolean\"}\n",
        "if auto_off:\n",
        "    print(\"\\033[96mAuto Off encendido\\033[0m\")\n",
        "if not auto_off:\n",
        "    print(\"\\033[96mAuto Off apagado\\033[0m\")\n",
        "\n",
        "\n",
        "# These carry information from past executions\n",
        "if \"old_model_url\" not in globals():\n",
        "  old_model_url = None\n",
        "if \"dependencies_installed\" not in globals():\n",
        "  dependencies_installed = False\n",
        "if \"model_file\" not in globals():\n",
        "  model_file = None\n",
        "if \"model_cached\" not in globals():\n",
        "  model_cached = False\n",
        "if \"diffusers_model\" not in globals():\n",
        "  diffusers_model = None\n",
        "\n",
        "# These may be set by other cells, some are legacy\n",
        "if \"custom_dataset\" not in globals():\n",
        "  custom_dataset = None\n",
        "if \"override_dataset_config_file\" not in globals():\n",
        "  override_dataset_config_file = None\n",
        "if \"continue_from_lora\" not in globals():\n",
        "  continue_from_lora = \"\"\n",
        "if \"override_config_file\" not in globals():\n",
        "  override_config_file = None\n",
        "\n",
        "COLAB = True\n",
        "SOURCE = \"https://github.com/uYouUs/sd-scripts\"\n",
        "BRANCH = None\n",
        "COMMIT = None\n",
        "BETTER_EPOCH_NAMES = True\n",
        "LOAD_TRUNCATED_IMAGES = True\n",
        "try:\n",
        "  LOWRAM = int(next(line.split()[1] for line in open('/proc/meminfo') if \"MemTotal\" in line)) / (1024**2) < 15\n",
        "except:\n",
        "  LOWRAM = False\n",
        "\n",
        "#@title ## üö© Empieza Aqu√≠\n",
        "\n",
        "#@markdown ### ‚ñ∂Ô∏è Configuraci√≥n\n",
        "#@markdown El nombre de tu proyecto tambi√©n es el nombre de la carpeta donde ir√°n tus im√°genes. No se permiten espacios.\n",
        "project_name = \"\" #@param {type:\"string\"}\n",
        "folder_structure = \"Organize by project (MyDrive/Loras/project_name/dataset)\" #@param [\"Organize by category (MyDrive/lora_training/datasets/project_name)\", \"Organize by project (MyDrive/Loras/project_name/dataset)\"]\n",
        "project_name = project_name.strip()\n",
        "\n",
        "\n",
        "#@markdown Decide el modelo base que se descargar√° y utilizar√° para la entrenar. Tambi√©n puedes elegir uno propio pegando su enlace de descarga, o uno en tu Google Drive que comience con /`content/drive/MyDrive`.\n",
        "training_model = \"Illustrious_2.0\" # @param [\"Pony Diffusion V6 XL\",\"Animagine XL V3\",\"animagine_4.0_zero\",\"Illustrious_0.1\",\"Illustrious_2.0\",\"NoobAI-XL0.75\",\"NoobAI-XL0.5\",\"Stable Diffusion XL 1.0 base\",\"NoobAIXL0_75vpred\",\"RouWei_v080vpred\"]\n",
        "optional_custom_training_model = \"\" #@param {type:\"string\"}\n",
        "#@markdown El uso de un modelo en diffusers utiliza menos recursos. obligatorio en colab gratis.\n",
        "load_diffusers = True #@param {type:\"boolean\"}\n",
        "force_dif = False if optional_custom_training_model and not load_diffusers else True # Built in models will force use diffuser versions as they handle caching between swapping models\n",
        "#@markdown Habilitar vprediction para modelos compatibles\n",
        "vpred = False #@param {type:\"boolean\"}\n",
        "#@markdown Civitai API token para modelos que requieren autorizaci√≥n.\n",
        "civitoken = \"\" #@param {type:\"string\"}\n",
        "#@markdown Usa wandb si quieres visualizar el progreso de tu entrenamiento a el paso del tiempo.\n",
        "wandb_key = \"\" #@param {type:\"string\"}\n",
        "model_url = None\n",
        "if optional_custom_training_model:\n",
        "  model_url = optional_custom_training_model\n",
        "  model_url = model_url.strip()\n",
        "  model_file = \"/content/custom_model.safetensors\"\n",
        "elif \"Pony\" in training_model:\n",
        "  diffusers_model = \"WhiteAiZ/Pony_diffusion_v6_diffusers_fp16\"\n",
        "elif \"Animagine\" in training_model:\n",
        "  idiffusers_model = \"cagliostrolab/animagine-xl-3.0\"\n",
        "elif \"animagine_4.0_zero\" in training_model:\n",
        "  diffusers_model = \"cagliostrolab/animagine-xl-4.0-zero\"\n",
        "elif \"Illustrious_0.1\" in training_model:\n",
        "  diffusers_model = \"OnomaAIResearch/Illustrious-xl-early-release-v0\"\n",
        "elif \"Illustrious_2.0\" in training_model:\n",
        "  diffusers_model = \"WhiteAiZ/Illustrious_2.0\"\n",
        "elif \"NoobAI-XL0.75\" in training_model:\n",
        "  diffusers_model = \"Laxhar/noobai-XL-0.75\"\n",
        "elif \"NoobAI-XL0.5\" in training_model:\n",
        "  diffusers_model = \"Laxhar/noobai-XL-0.5\"\n",
        "elif \"Stable Diffusion XL 1.0 base\" in training_model:\n",
        "  if load_diffusers:\n",
        "    diffusers_model = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
        "elif \"NoobAIXL0_75vpred\" in training_model:\n",
        "  vpred = True\n",
        "  diffusers_model = \"Laxhar/noobai-XL-Vpred-0.75\"\n",
        "else:\n",
        "  vpred = True\n",
        "  if load_diffusers:\n",
        "    diffusers_model = \"John6666/rouwei-v080-vpred-sdxl\"\n",
        "\n",
        "if load_diffusers:\n",
        "  vae_file= \"stabilityai/sdxl-vae\"\n",
        "\n",
        "#@markdown ### ‚ñ∂Ô∏è Procesamiento\n",
        "#@markdown Por defecto la resoluci√≥n para personajes es 1024. otras resoluciones que puedes usar son 896 (recomendado para personajes o 1024) y 768 (recomendado para estilos, puedes usar m√°s repeticiones con esta resoluci√≥n).\n",
        "resolution = 1024 #@param {type:\"dropdown\", min:768, max:1536, step:128}\n",
        "#@markdown Activa `Flip Aug`si tu dataset es peque√±o, util en personajes isometricos, volteara todas tus imagenes (modo espejo) para aprender el doble, pero podria afectar a personajes con tatuajes, marcas, cicatrices etc...\n",
        "flip_aug = False #@param {type:\"boolean\"}\n",
        "caption_extension = \".txt\" #@param [\".txt\", \".caption\"]\n",
        "#@markdown Mezclar las etiquetas ayuda al aprendizaje. Las etiquetas de activaci√≥n van al inicio de cada archivo de texto y no se mezclar√°n.<p>\n",
        "shuffle_tags = True #@param {type:\"boolean\"}\n",
        "activation_tags = \"1\" #@param [0,1,2,3]\n",
        "keep_tokens = int(activation_tags)\n",
        "\n",
        "#@markdown ### ‚ñ∂Ô∏è Pasos <p>\n",
        "#@markdown Tus im√°genes se repetir√°n este n√∫mero de veces durante el entrenamiento. Recomiendo que el valor total sea entre 200 y 400.\n",
        "num_repeats = 1 #@param {type:\"number\"}\n",
        "#@markdown Cu√°nto tiempo deseas entrenar. Un buen punto de partida puede ser alrededor de 10 √©pocas o alrededor de 2000 pasos.<p>\n",
        "#@markdown Una √©poca es una cantidad de pasos igual a tu cantidad de im√°genes multiplicada por sus repeticiones, y dividido en el batch size.<p>\n",
        "preferred_unit = \"Epochs\" #@param [\"Epochs\", \"Steps\"]\n",
        "how_many = 10 #@param {type:\"number\"}\n",
        "max_train_epochs = how_many if preferred_unit == \"Epochs\" else None\n",
        "max_train_steps = how_many if preferred_unit == \"Steps\" else None\n",
        "#@markdown Guardar m√°s √©pocas te permitir√° comparar mejor el progreso de tu Lora a cambio de necesitar m√°s espacio de almacenamiento.\n",
        "save_every_n_epochs = 1 #@param {type:\"number\"}\n",
        "keep_only_last_n_epochs = 10 #@param {type:\"number\"}\n",
        "if not save_every_n_epochs:\n",
        "  save_every_n_epochs = max_train_epochs\n",
        "if not keep_only_last_n_epochs:\n",
        "  keep_only_last_n_epochs = max_train_epochs\n",
        "\n",
        "#@markdown ### ‚ñ∂Ô∏è Aprendizaje\n",
        "#@markdown La tasa de aprendizaje es lo m√°s importante. Si deseas entrenar m√°s lento con muchas im√°genes, o si tienes un dim y alpha altos, usa un unet y text_enconder de 1e-4 y 1e-5 respectivamente, o incluso m√°s bajo. <p>\n",
        "#@markdown El text encoder ayuda a tu Lora a aprender conceptos un poco mejor. Se recomienda la mitad o un quinto del unet. Puedes dejarlo en 0 para algunos estilos.\n",
        "unet_lr = 3e-4 #@param {type:\"number\"}\n",
        "text_encoder_lr = 6e-5 #@param {type:\"number\"}\n",
        "#@markdown El scheduler es el algoritmo matem√°tico que guiar√° el entrenamiento. Para personajes recomiendo `cosin_whit_restarts` con un valor de 3. Si no est√°s seguro ponlo en `constant` e ignora el valor.\n",
        "lr_scheduler = \"cosine_with_restarts\" #@param [\"constant\", \"cosine\", \"cosine_with_restarts\", \"constant_with_warmup\", \"linear\", \"polynomial\"]\n",
        "lr_scheduler_number = 3 #@param {type:\"number\"}\n",
        "#@markdown Pasos de \"calentamiento\" durante el entrenamiento para un inicio eficiente. Recomiendo dejarlo en 5%.\n",
        "lr_warmup_ratio = 0.05 #@param {type:\"slider\", min:0.0, max:0.2, step:0.01}\n",
        "lr_warmup_steps = 0 #@param {type: \"number\", min:0.0, max:100, step:0.0}\n",
        "#@markdown Ajusta la p√©rdida con el tiempo, hace que el aprendizaje sea mucho m√°s eficiente. El papel recomienda 5.0, yo recomiendo 8.0 para anime. Un valor m√°s alto es menos estricto. Usa 0 para desactivarlo.\n",
        "min_snr_gamma = 8.0 #@param {type:\"slider\", min:0.0, max:16.0, step:0.5}\n",
        "#@markdown Multinoise puede ayudar con el equilibrio del color (negros m√°s oscuros, blancos m√°s claros). Pero puede da√±ar la generaci√≥n de ojos, usalo solo si es necesario. No es necesario activar si se entrena modelo Vpred.\n",
        "multinoise = True #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ### ‚ñ∂Ô∏è Estructura\n",
        "#@markdown LoRA es del tipo cl√°sico y bueno para una variedad de prop√≥sitos. LoCon es bueno con los estilos art√≠sticos ya que tiene m√°s capas para aprender m√°s aspectos del dataset.\n",
        "lora_type = \"LoRA\" #@param [\"LoRA\", \"LoCon\"]\n",
        "\n",
        "#@markdown Abajo estan los valores recomendados para las siguientes configuraciones:\n",
        "\n",
        "#@markdown | type | network_dim | network_alpha | conv_dim | conv_alpha |\n",
        "#@markdown | :---: | :---: | :---: | :---: | :---: |\n",
        "#@markdown | Regular LoRA | 8 | 4 |   |   |\n",
        "#@markdown | Style LoCon | 16 | 8 | 16 | 8 |\n",
        "\n",
        "#@markdown Un dim mayor crea a una Lora m√°s grande, pero no siempre es mejor. Aumenta solo si usas un dataset grande.\n",
        "network_dim = 8 #@param {type:\"number\", min:1, max:32, step:1}\n",
        "network_alpha = 16 #@param {type:\"number\", min:1, max:32, step:1}\n",
        "#@markdown Los siguientes dos valores solo aplican a las capas adicionales de LoCon.\n",
        "conv_dim = 16 #@param {type:\"number\", min:1, max:32, step:1}\n",
        "conv_alpha = 8 #@param {type:\"number\", min:1, max:32, step:1}\n",
        "\n",
        "network_module = \"networks.lora\"\n",
        "network_args = None\n",
        "if lora_type.lower() == \"locon\":\n",
        "  network_args = [f\"conv_dim={conv_dim}\", f\"conv_alpha={conv_alpha}\"]\n",
        "\n",
        "#@markdown ### ‚ñ∂Ô∏è Entrenamiento\n",
        "#@markdown Ajusta estos par√°metros seg√∫n tu configuraci√≥n de colab.\n",
        "#@markdown Si est√°s utilizando los recursos gratuito, debes seleccionar un modelo de formato diffusers en la parte superior de esta celda.\n",
        "#@markdown\n",
        "#@markdown Un tama√±o de lote `train_batch_size` mayor suele ser m√°s r√°pido, pero utiliza m√°s memoria.\n",
        "train_batch_size = 4 #@param {type:\"slider\", min:1, max:16, step:1}\n",
        "#markdown No hay una diferencia sustancial entre sdpa y xformers.\n",
        "cross_attention = \"sdpa\" #param [\"sdpa\", \"xformers\"]\n",
        "#@markdown Si est√°s usando una A100, deberias usar bf16. Si se est√° quedando sin memoria en los recursos gratuitos con un modelo sin formato diffusers, prueba con `full_fp16`.\n",
        "mixed_precision = \"fp16\" #@param [\"bf16\", \"fp16\", \"full_fp16\"]\n",
        "#@markdown `cache_latents_to_drive` agregar√° un archivo de 250 KB junto a cada imagen, pero utilizar√° considerablemente menos memoria.\n",
        "cache_latents = True #@param {type:\"boolean\"}\n",
        "cache_latents_to_drive = True #@param {type:\"boolean\"}\n",
        "#@markdown La siguiente opci√≥n desactivar√° el mezclado de etiquetas `shuffle_tags` y deshabilitar√° el entrenamiento del codificador de texto.\n",
        "cache_text_encoder_outputs  = False  # @param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ### ‚ñ∂Ô∏è Avanzado (Opcional)\n",
        "#@markdown El optimizador es el algoritmo utilizado para el entrenamiento. `AdamW8bit` es el predeterminado y funciona muy bien, mientras que `Prodigy` administra la tasa de aprendizaje autom√°ticamente y puede tener varias ventajas, como entrenar m√°s r√°pido debido a que necesita menos pasos, as√≠ como trabajar mejor para conjuntos de datos peque√±os.\n",
        "#@markdown para entrenamiento de `PonyV6` recomiendo usar `Adafactor`. Mientras que `Illustrious` Funciona mejor con `AdamW8bit`\n",
        "optimizer = \"AdamW8bit\" #@param [\"AdamW8bit\", \"Prodigy\", \"DAdaptation\", \"DadaptAdam\", \"DadaptLion\", \"AdamW\", \"Lion\", \"SGDNesterov\", \"SGDNesterov8bit\", \"AdaFactor\"]\n",
        "#@markdown Argumentos recomendados para AdamW8bit: `weight_decay=0.1 betas=[0.9,0.99]`<p>\n",
        "#@markdown Argumentos recomendados para Prodigy: `decouple=True weight_decay=0.01 betas=[0.9,0.999] d_coef=2 use_bias_correction=True safeguard_warmup=True`<p>\n",
        "#@markdown Argumentos recomendados para Adafactor: `scale_parameter=False relative_step=False warmup_init=False`\n",
        "optimizer_args = \"weight_decay=0.1 betas=[0.9,0.99]\" #@param {type:\"string\"}\n",
        "optimizer_args = [a.strip() for a in optimizer_args.split(' ') if a]\n",
        "#@markdown Si se selecciona Dadapt o Prodigy y se marca la casilla de abajo, los siguientes valores recomendados reemplazaran la configuraci√≥n anterior:<p>\n",
        "#@markdown `unet_lr=1`, `text_encoder_lr=1`, `network_alpha=network_dim`\n",
        "recommended_values = True #@param {type:\"boolean\"}\n",
        "\n",
        "if any(opt in optimizer.lower() for opt in [\"dadapt\", \"prodigy\"]):\n",
        "  if recommended_values:\n",
        "    unet_lr = 1\n",
        "    text_encoder_lr = 1\n",
        "    network_alpha = network_dim\n",
        "\n",
        "lr_scheduler_num_cycles = lr_scheduler_number if lr_scheduler == \"cosine_with_restarts\" else 0\n",
        "lr_scheduler_power = lr_scheduler_number if lr_scheduler == \"polynomial\" else 0\n",
        "\n",
        "#@markdown ### ‚ñ∂Ô∏è Generar imagen de muestra (Opcional)\n",
        "#@markdown Esto permite supervisar el entrenamiento.<p>\n",
        "#@markdown Genera una imagen de muestra con la Lora despu√©s de cada √©poca guardada usando el siguiente prompt:<p>\n",
        "#@markdown Las im√°genes se guardar√°n en `GDrive/Loras/project_name/output/samples`<p>\n",
        "#@markdown Agrega opciones de generaci√≥n opcionales al final usando `--w {width} --h {height} --d {seed} --s {steps} --l {cfg}`<p>\n",
        "#@markdown Prompt de ejemplo: `1girl, blonde hair, smiling --w 1024 --h 1024 --d 173371316 --s 20 --l 5`<p>\n",
        "#@markdown Esta opcion usa m√°s VRAM al generar la imagen. El entrenamiento se puede alentar despu√©s de generar una imagen, debido a que se mueven cosas de afuera la VRAM. No se use si deseas maximizar el tiempo de entrenamiento.\n",
        "sample_prompt = \"\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ### ‚ñ∂Ô∏è Listo\n",
        "#@markdown Ahora puedes correr esta celda apretando el bot√≥n circular a la izquierda. ¬°Buena suerte! <p>\n",
        "\n",
        "\n",
        "# üë©‚Äçüíª Cool code goes here\n",
        "\n",
        "root_dir = \"/content\" if COLAB else pathlib.Path.home() / \"Loras\"\n",
        "deps_dir = os.path.join(root_dir, \"deps\")\n",
        "repo_dir = os.path.join(root_dir, \"kohya-trainer\")\n",
        "\n",
        "if \"/Loras\" in folder_structure:\n",
        "  main_dir      = os.path.join(root_dir, \"drive/MyDrive/Loras\") if COLAB else root_dir\n",
        "  log_folder    = os.path.join(main_dir, \"_logs\")\n",
        "  config_folder = os.path.join(main_dir, project_name)\n",
        "  images_folder = os.path.join(main_dir, project_name, \"dataset\")\n",
        "  output_folder = os.path.join(main_dir, project_name, \"output\")\n",
        "else:\n",
        "  main_dir      = os.path.join(root_dir, \"drive/MyDrive/lora_training\") if COLAB else root_dir\n",
        "  images_folder = os.path.join(main_dir, \"datasets\", project_name)\n",
        "  output_folder = os.path.join(main_dir, \"output\", project_name)\n",
        "  config_folder = os.path.join(main_dir, \"config\", project_name)\n",
        "  log_folder    = os.path.join(main_dir, \"log\")\n",
        "\n",
        "config_file = os.path.join(config_folder, \"training_config.toml\")\n",
        "dataset_config_file = os.path.join(config_folder, \"dataset_config.toml\")\n",
        "accelerate_config_file = os.path.join(repo_dir, \"accelerate_config/config.yaml\")\n",
        "\n",
        "def install_dependencies():\n",
        "  if 'installed' not in globals():\n",
        "    !wget --show-progress -q https://github.com/camenduru/gperftools/releases/download/v1.0/libtcmalloc_minimal.so.4 -O /content/libtcmalloc_minimal.so.4\n",
        "    installed = True\n",
        "  print(\"Instalando entrenador\")\n",
        "  os.chdir(root_dir)\n",
        "  !git clone {SOURCE} {repo_dir}\n",
        "  os.chdir(repo_dir)\n",
        "  if BRANCH:\n",
        "    !git checkout {BRANCH}\n",
        "  if COMMIT:\n",
        "    !git reset --hard {COMMIT}\n",
        "  !wget --show-progress -q https://raw.githubusercontent.com/uYouUs/kohya-colab/main/train_network_xl_wrapper.py -q -O train_network_xl_wrapper.py\n",
        "  !wget --show-progress -q https://raw.githubusercontent.com//uYouUs/kohya-colab/main/dracula.py -q -O dracula.py\n",
        "\n",
        "  !apt -y update -qq\n",
        "  !apt -y install aria2 -qq\n",
        "\n",
        "  !pip install uv\n",
        "  #!pip install accelerate==1.2.1 opencv-python==4.10.0.84 einops==0.8.0 \\ #debug, slows down startup if active\n",
        "  !uv pip install bitsandbytes==0.45.1 pytorch-lightning==1.9.0 voluptuous==0.13.1 \\\n",
        "    invisible-watermark==0.2.0 prodigyopt==1.0.0 \\\n",
        "    dadaptation==3.1 lion-pytorch==0.1.2 ftfy==6.1.1 --no-progress\n",
        "    # toml==0.10.2 safetensors pygments wandb imagesize==1.4.1 #debug\n",
        "  !uv pip install -e . --no-progress\n",
        "  if cross_attention == \"xformers\":\n",
        "    !uv pip install -q xformers==0.0.29.post3 --no-progress\n",
        "\n",
        "  # patch kohya for minor stuff\n",
        "  if LOWRAM:\n",
        "    !sed -i \"s@cpu@cuda@\" library/model_util.py\n",
        "  if LOAD_TRUNCATED_IMAGES:\n",
        "    !sed -i 's/from PIL import Image/from PIL import Image, ImageFile\\nImageFile.LOAD_TRUNCATED_IMAGES=True/g' library/train_util.py # fix truncated jpegs error\n",
        "  if BETTER_EPOCH_NAMES:\n",
        "    !sed -i 's/{:06d}/{:02d}/g' library/train_util.py # make epoch names shorter\n",
        "    !sed -i 's/\".\" + args.save_model_as)/\"-{:02d}.\".format(num_train_epochs) + args.save_model_as)/g' train_network.py # name of the last epoch will match the rest\n",
        "\n",
        "  from accelerate.utils import write_basic_config\n",
        "  if not os.path.exists(accelerate_config_file):\n",
        "    write_basic_config(save_location=accelerate_config_file)\n",
        "\n",
        "  os.environ[\"LD_PRELOAD\"] = \"/content/libtcmalloc_minimal.so.4\"\n",
        "  os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "  os.environ[\"BITSANDBYTES_NOWELCOME\"] = \"1\"\n",
        "  os.environ[\"SAFETENSORS_FAST_GPU\"] = \"1\"\n",
        "  os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n",
        "\n",
        "def validate_dataset():\n",
        "  global lr_warmup_steps, lr_warmup_ratio, caption_extension, keep_tokens, model_url\n",
        "  supported_types = (\".png\", \".jpg\", \".jpeg\", \".webp\", \".bmp\")\n",
        "\n",
        "  print(\"\\nüíø Checking dataset...\")\n",
        "  if not project_name.strip() or any(c in project_name for c in \" .()\\\"'\\\\/\"):\n",
        "    print(\"üí• Error: Please choose a valid project name.\")\n",
        "    return\n",
        "\n",
        "  # Find the folders and files\n",
        "  if custom_dataset:\n",
        "    try:\n",
        "      datconf = toml.loads(custom_dataset)\n",
        "      datasets = [d for d in datconf[\"datasets\"][0][\"subsets\"]]\n",
        "    except:\n",
        "      print(f\"üí• Error: Your custom dataset is invalid or contains an error! Please check the original template.\")\n",
        "      return\n",
        "    reg = [d.get(\"image_dir\") for d in datasets if d.get(\"is_reg\", False)]\n",
        "    datasets_dict = {d[\"image_dir\"]: d[\"num_repeats\"] for d in datasets}\n",
        "    folders = datasets_dict.keys()\n",
        "    files = [f for folder in folders for f in os.listdir(folder)]\n",
        "    images_repeats = {folder: (len([f for f in os.listdir(folder) if f.lower().endswith(supported_types)]), datasets_dict[folder]) for folder in folders}\n",
        "  else:\n",
        "    reg = []\n",
        "    folders = [images_folder]\n",
        "    files = os.listdir(images_folder)\n",
        "    images_repeats = {images_folder: (len([f for f in files if f.lower().endswith(supported_types)]), num_repeats)}\n",
        "\n",
        "  # Validation\n",
        "  for folder in folders:\n",
        "    if not os.path.exists(folder):\n",
        "      print(f\"üí• Error: The folder {folder.replace('/content/drive/', '')} doesn't exist.\")\n",
        "      return\n",
        "  for folder, (img, rep) in images_repeats.items():\n",
        "    if not img:\n",
        "      print(f\"üí• Error: Your {folder.replace('/content/drive/', '')} folder is empty.\")\n",
        "      return\n",
        "  test_files = []\n",
        "  for f in files:\n",
        "    if not f.lower().endswith((caption_extension, \".npz\")) and not f.lower().endswith(supported_types):\n",
        "      print(f\"üí• Error: Invalid file in dataset: \\\"{f}\\\". Aborting.\")\n",
        "      return\n",
        "    for ff in test_files:\n",
        "      if f.endswith(supported_types) and ff.endswith(supported_types) \\\n",
        "          and os.path.splitext(f)[0] == os.path.splitext(ff)[0]:\n",
        "        print(f\"üí• Error: The files {f} and {ff} cannot have the same name. Aborting.\")\n",
        "        return\n",
        "    test_files.append(f)\n",
        "\n",
        "  if caption_extension and not [txt for txt in files if txt.lower().endswith(caption_extension)]:\n",
        "    caption_extension = \"\"\n",
        "  if continue_from_lora and not (continue_from_lora.endswith(\".safetensors\") and os.path.exists(continue_from_lora)):\n",
        "    print(f\"üí• Error: Invalid path to existing Lora. Example: /content/drive/MyDrive/Loras/example.safetensors\")\n",
        "    return\n",
        "\n",
        "  # Pretty stuff\n",
        "\n",
        "  pre_steps_per_epoch = sum(img*rep for (img, rep) in images_repeats.values())\n",
        "  steps_per_epoch = pre_steps_per_epoch/train_batch_size\n",
        "  total_steps = max_train_steps or int(max_train_epochs*steps_per_epoch)\n",
        "  estimated_epochs = int(total_steps/steps_per_epoch)\n",
        "  lr_warmup_steps = int(total_steps*lr_warmup_ratio)\n",
        "\n",
        "  for folder, (img, rep) in images_repeats.items():\n",
        "    print(\"üìÅ\"+folder.replace(\"/content/drive/\", \"\") + (\" (Regularization)\" if folder in reg else \"\"))\n",
        "    print(f\"üìà Found {img} images with {rep} repeats, equaling {img*rep} steps.\")\n",
        "  print(f\"üìâ Divide {pre_steps_per_epoch} steps by {train_batch_size} batch size to get {steps_per_epoch} steps per epoch.\")\n",
        "  if max_train_epochs:\n",
        "    print(f\"üîÆ There will be {max_train_epochs} epochs, for around {total_steps} total training steps.\")\n",
        "  else:\n",
        "    print(f\"üîÆ There will be {total_steps} steps, divided into {estimated_epochs} epochs and then some.\")\n",
        "\n",
        "  if total_steps > 10000:\n",
        "    print(\"üí• Error: Your total steps are too high. You probably made a mistake. Aborting...\")\n",
        "    return\n",
        "\n",
        "  return True\n",
        "\n",
        "def create_config():\n",
        "  global dataset_config_file, config_file, model_file\n",
        "\n",
        "  if override_config_file:\n",
        "    config_file = override_config_file\n",
        "    print(f\"\\n‚≠ï Using custom config file {config_file}\")\n",
        "  else:\n",
        "    config_dict = {\n",
        "      \"network_arguments\": {\n",
        "        \"unet_lr\": unet_lr,\n",
        "        \"text_encoder_lr\": text_encoder_lr if not cache_text_encoder_outputs else 0,\n",
        "        \"network_dim\": network_dim,\n",
        "        \"network_alpha\": network_alpha,\n",
        "        \"network_module\": network_module,\n",
        "        \"network_args\": network_args,\n",
        "        \"network_train_unet_only\": text_encoder_lr == 0 or cache_text_encoder_outputs,\n",
        "        \"network_weights\": continue_from_lora if continue_from_lora else None\n",
        "      },\n",
        "      \"optimizer_arguments\": {\n",
        "        \"learning_rate\": unet_lr,\n",
        "        \"lr_scheduler\": lr_scheduler,\n",
        "        \"lr_scheduler_num_cycles\": lr_scheduler_num_cycles if lr_scheduler == \"cosine_with_restarts\" else None,\n",
        "        \"lr_scheduler_power\": lr_scheduler_power if lr_scheduler == \"polynomial\" else None,\n",
        "        \"lr_warmup_steps\": lr_warmup_steps if lr_scheduler != \"constant\" else None,\n",
        "        \"optimizer_type\": optimizer,\n",
        "        \"optimizer_args\": optimizer_args if optimizer_args else None,\n",
        "      },\n",
        "      \"training_arguments\": {\n",
        "        \"pretrained_model_name_or_path\": model_file,\n",
        "        \"vae\": vae_file,\n",
        "        \"max_train_steps\": max_train_steps,\n",
        "        \"max_train_epochs\": max_train_epochs,\n",
        "        \"train_batch_size\": train_batch_size,\n",
        "        \"seed\": 42,\n",
        "        \"max_token_length\": 225,\n",
        "        \"xformers\": cross_attention == \"xformers\",\n",
        "        \"sdpa\": cross_attention == \"sdpa\",\n",
        "        \"min_snr_gamma\": min_snr_gamma if min_snr_gamma > 0 else None,\n",
        "        \"lowram\": LOWRAM,\n",
        "        \"no_half_vae\": True,\n",
        "        \"gradient_checkpointing\": True,\n",
        "        \"gradient_accumulation_steps\": 1,\n",
        "        \"max_data_loader_n_workers\": 3,\n",
        "        \"persistent_data_loader_workers\": True,\n",
        "        \"mixed_precision\": \"fp16\" if mixed_precision == \"full_fp16\" else mixed_precision,\n",
        "        \"full_bf16\": mixed_precision == \"bf16\",\n",
        "        \"full_fp16\": mixed_precision == \"full_fp16\",\n",
        "        \"cache_latents\": cache_latents,\n",
        "        \"cache_latents_to_disk\": cache_latents_to_drive,\n",
        "        \"cache_text_encoder_outputs\": cache_text_encoder_outputs,\n",
        "        \"min_timestep\": 0,\n",
        "        \"max_timestep\": 1000,\n",
        "        \"prior_loss_weight\": 1.0,\n",
        "        \"multires_noise_iterations\": 6 if multinoise else None,\n",
        "        \"multires_noise_discount\": 0.3 if multinoise else None,\n",
        "        \"v_parameterization\": vpred,\n",
        "        \"scale_v_pred_loss_like_noise_pred\": vpred,\n",
        "        \"zero_terminal_snr\": vpred,\n",
        "      },\n",
        "      \"saving_arguments\": {\n",
        "        \"save_precision\": \"fp16\",\n",
        "        \"save_model_as\": \"safetensors\",\n",
        "        \"save_every_n_epochs\": save_every_n_epochs,\n",
        "        \"save_last_n_epochs\": keep_only_last_n_epochs,\n",
        "        \"output_name\": project_name,\n",
        "        \"output_dir\": output_folder,\n",
        "        \"log_prefix\": project_name,\n",
        "        \"logging_dir\": log_folder,\n",
        "        \"wandb_api_key\": wandb_key if wandb_key else None,\n",
        "        \"log_with\": \"wandb\" if wandb_key else None,\n",
        "        \"sample_prompts\": prompt_file if sample_prompt else None,\n",
        "        \"sample_every_n_epochs\": save_every_n_epochs if sample_prompt else None,\n",
        "        \"sample_sampler\": \"euler_a\" if sample_prompt else None,\n",
        "      }\n",
        "    }\n",
        "\n",
        "    for key in config_dict:\n",
        "      if isinstance(config_dict[key], dict):\n",
        "        config_dict[key] = {k: v for k, v in config_dict[key].items() if v is not None}\n",
        "\n",
        "    with open(config_file, \"w\") as f:\n",
        "      f.write(toml.dumps(config_dict))\n",
        "    print(f\"\\nüìÑ Config saved to {config_file}\")\n",
        "\n",
        "  if override_dataset_config_file:\n",
        "    dataset_config_file = override_dataset_config_file\n",
        "    print(f\"‚≠ï Using custom dataset config file {dataset_config_file}\")\n",
        "  else:\n",
        "    dataset_config_dict = {\n",
        "      \"general\": {\n",
        "        \"resolution\": resolution,\n",
        "        \"shuffle_caption\": shuffle_tags and not cache_text_encoder_outputs,\n",
        "        \"keep_tokens\": keep_tokens,\n",
        "        \"flip_aug\": flip_aug,\n",
        "        \"caption_extension\": caption_extension,\n",
        "        \"enable_bucket\": True,\n",
        "        \"bucket_no_upscale\": False,\n",
        "        \"bucket_reso_steps\": 64,\n",
        "        \"min_bucket_reso\": 256,\n",
        "        \"max_bucket_reso\": 4096,\n",
        "      },\n",
        "      \"datasets\": toml.loads(custom_dataset)[\"datasets\"] if custom_dataset else [\n",
        "        {\n",
        "          \"subsets\": [\n",
        "            {\n",
        "              \"num_repeats\": num_repeats,\n",
        "              \"image_dir\": images_folder,\n",
        "              \"class_tokens\": None if caption_extension else project_name\n",
        "            }\n",
        "          ]\n",
        "        }\n",
        "      ]\n",
        "    }\n",
        "\n",
        "\n",
        "    for key in dataset_config_dict:\n",
        "      if isinstance(dataset_config_dict[key], dict):\n",
        "        dataset_config_dict[key] = {k: v for k, v in dataset_config_dict[key].items() if v is not None}\n",
        "\n",
        "    with open(dataset_config_file, \"w\") as f:\n",
        "      f.write(toml.dumps(dataset_config_dict))\n",
        "    print(f\"üìÑ Dataset config saved to {dataset_config_file}\")\n",
        "\n",
        "def download_model():\n",
        "  global old_model_url, model_url, model_file\n",
        "  real_model_url = model_url  # Keep model url intact to compare against old model url to detect a change.\n",
        "\n",
        "  # Test for google drive model\n",
        "  if real_model_url.startswith(\"/content/drive/\"):\n",
        "    model_file = real_model_url\n",
        "    if model_file.lower().endswith(\".safetensors\") or model_file.lower().endswith(\".ckpt\"):\n",
        "      print(f\"\\nTraing will be done with drive model at {model_file}, check it is valid if you recieve errors\\n\")\n",
        "      return True\n",
        "    return False\n",
        "\n",
        "  else:\n",
        "    # Define local filename\n",
        "    if not model_file or not model_file.endswith((\".ckpt\", \".safetensors\")):\n",
        "      if real_model_url.lower().endswith((\".ckpt\", \".safetensors\")):\n",
        "        model_file = f\"/content{real_model_url[real_model_url.rfind('/'):]}\"\n",
        "      else:\n",
        "        model_file = \"/content/downloaded_model.safetensors\"\n",
        "        if os.path.exists(model_file):\n",
        "          !rm \"{model_file}\"\n",
        "\n",
        "    # HuggingFace\n",
        "    if m := re.search(r\"(?:https?://)?(?:www\\.)?huggingface\\.co/[^/]+/[^/]+/blob\", real_model_url):\n",
        "      real_model_url = real_model_url.replace(\"blob\", \"resolve\")\n",
        "    # Civitai\n",
        "    elif m := re.search(r\"(?:https?://)?(?:www\\\\.)?civitai\\.com/models/([0-9]+)(/[A-Za-z0-9-_]+)?\", real_model_url):\n",
        "      if m.group(2):\n",
        "        model_file = f\"/content{m.group(2)}.safetensors\"\n",
        "      if m := re.search(r\"modelVersionId=([0-9]+)\", real_model_url):\n",
        "        real_model_url = f\"https://civitai.com/api/download/models/{m.group(1)}\"\n",
        "      else:\n",
        "        raise ValueError(\"üí• optional_custom_training_model contains a civitai link, but the link doesn't include a modelVersionId. You can also right click the download button to copy the direct download link.\")\n",
        "      if civitoken:\n",
        "        real_model_url = real_model_url+\"?token=\"+civitoken\n",
        "    # Download checkpoint\n",
        "    !aria2c \"{real_model_url}\" --console-log-level=warn -c -s 16 -x 16 -k 10M -d / -o \"{model_file}\"\n",
        "\n",
        "    # Validation\n",
        "\n",
        "    if model_file.lower().endswith(\".safetensors\"):\n",
        "      from safetensors.torch import load_file as load_safetensors\n",
        "      try:\n",
        "        test = load_safetensors(model_file)\n",
        "        del test\n",
        "      except:\n",
        "        new_model_file = os.path.splitext(model_file)[0]+\".ckpt\"\n",
        "        !mv \"{model_file}\" \"{new_model_file}\"\n",
        "        model_file = new_model_file\n",
        "        print(f\"Renamed model to {os.path.splitext(model_file)[0]}.ckpt\")\n",
        "\n",
        "    if model_file.lower().endswith(\".ckpt\"):\n",
        "      from torch import load as load_ckpt\n",
        "      try:\n",
        "        test = load_ckpt(model_file)\n",
        "        del test\n",
        "      except:\n",
        "        return False\n",
        "\n",
        "  return True\n",
        "\n",
        "def getModel():\n",
        "  global diffusers_model, model_url, model_file, old_model_url\n",
        "  #!pip install transformers==4.47.1 diffusers==0.32.2 jax==0.4.33 jaxlib==0.4.33 huggingface_hub==0.27.1 flax==0.10.2\n",
        "  from huggingface_hub import snapshot_download\n",
        "  from huggingface_hub.utils import disable_progress_bars\n",
        "  disable_progress_bars() # Disable model download progress bars to clean up output from threading mess\n",
        "  snapshot_download(repo_id=vae_file, allow_patterns=[\"*model.safetensors\", \"*.json\"]) # Download Vae\n",
        "  if force_dif:\n",
        "    if optional_custom_training_model and 'huggingface.co' in model_url: # Strip url into repo id\n",
        "          match = re.search(r'huggingface\\.co/([^/]+)/([^/]+)', model_url)\n",
        "          if match:\n",
        "              username = match.group(1)\n",
        "              model_name = match.group(2)\n",
        "              diffusers_model = f\"{username}/{model_name}\"\n",
        "    elif optional_custom_training_model: # Already in repo id format\n",
        "      diffusers_model = model_url\n",
        "    model_file = diffusers_model\n",
        "    try:\n",
        "      print(\"\\nüîÑ Getting Diffuser model...\")\n",
        "      snapshot_download(repo_id=diffusers_model, allow_patterns=[\"*model.safetensors\", \"*.json\", \"*.txt\"]) # Much faster but runs into chance of downloading unnecessary files, slowing it down in rare cases.\n",
        "    except:\n",
        "      raise ValueError(f\"\\nInvalid Diffuser Model {diffusers_model}\")\n",
        "  elif old_model_url != model_url or not model_file or not os.path.exists(model_file):\n",
        "    print(\"\\nüîÑ Getting model...\")\n",
        "    if not download_model():\n",
        "      print(\"\\nüí• Error: The model you specified is invalid or corrupted.\"\n",
        "            \"\\nIf you're using an URL, please check that the model is accessible without being logged in.\"\n",
        "            \"\\nYou can try civitai or huggingface URLs, or a path in your Google Drive starting with /content/drive/MyDrive\")\n",
        "      return\n",
        "    print()\n",
        "  else:\n",
        "    print(\"\\nüîÑ Model already downloaded.\\n\")\n",
        "  old_model_url = model_url # only update old model url if we successfully downloaded it to prevent not downloading model if it previously failed on a change.\n",
        "\n",
        "def generateSample(): # Generate required files for sample generation\n",
        "  global output_folder, sample_prompt, prompt_file\n",
        "  if sample_prompt:\n",
        "    prompt_file = os.path.abspath(os.path.join(output_folder, \"sample\"))\n",
        "    if not os.path.exists(prompt_file): # Make sample directory so prompt file creation does not fail.\n",
        "      os.mkdir(prompt_file)\n",
        "    prompt_file = os.path.join(prompt_file, \"prompt.txt\")\n",
        "    with open(prompt_file, \"w\") as f:\n",
        "      f.write(sample_prompt)\n",
        "\n",
        "def thInstallDep():\n",
        "  global dependencies_installed\n",
        "  print(\"\\nüè≠ Installing dependencies...\\n\")\n",
        "  t0 = time()\n",
        "  install_dependencies()\n",
        "  t1 = time()\n",
        "  dependencies_installed = True\n",
        "  print(f\"\\n‚úÖ Installation finished in {int(t1-t0)} seconds. \\nWaiting on Model...\\n\")\n",
        "\n",
        "def main():\n",
        "  global dependencies_installed, sample_prompt\n",
        "\n",
        "  if COLAB and not os.path.exists('/content/drive'):\n",
        "    from google.colab import drive\n",
        "    print(\"üìÇ Connecting to Google Drive...\")\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "  threadList = []\n",
        "  # Download model with a thread\n",
        "  thr1 = threading.Thread(target = getModel)\n",
        "  threadList.append(thr1)\n",
        "  thr1.start()\n",
        "\n",
        "  if not dependencies_installed: # Begin installing dependencies while user gives gdrive permissions\n",
        "    thr2 = threading.Thread(target = thInstallDep)\n",
        "    threadList.append(thr2)\n",
        "    thr2.start()\n",
        "  else:\n",
        "    print(\"\\n‚úÖ Dependencies already installed.\")\n",
        "\n",
        "\n",
        "  for th in threadList: # Make sure model and or set are done before continuing\n",
        "    th.join()\n",
        "\n",
        "\n",
        "  for dir in (main_dir, deps_dir, repo_dir, log_folder, images_folder, output_folder, config_folder):\n",
        "    os.makedirs(dir, exist_ok=True)\n",
        "\n",
        "  if not validate_dataset():\n",
        "    return\n",
        "\n",
        "  generateSample() # Make sure sample file exists prior to creating config file\n",
        "  create_config()\n",
        "\n",
        "  print(\"\\n‚≠ê Starting trainer...\\n\")\n",
        "  os.chdir(repo_dir)\n",
        "\n",
        "  !accelerate launch --quiet --config_file={accelerate_config_file} --num_cpu_threads_per_process=2 --num_processes=2 train_network_xl_wrapper.py --dataset_config={dataset_config_file} --config_file={config_file}\n",
        "\n",
        "  if not get_ipython().__dict__['user_ns']['_exit_code']:\n",
        "    display(Markdown(\"### ‚úÖ Done! [Go download your Lora from Google Drive](https://drive.google.com/drive/my-drive)\\n\"\n",
        "                     \"### There will be several files, you should try the latest version (the file with the largest number next to it)\"))\n",
        "\n",
        "main()\n",
        "\n",
        "#auto off\n",
        "if auto_off:\n",
        "    print(\"\\033[96mAuto Off encendido, el entorno se desconectara en 30 segundos\\033[0m\")\n",
        "    from google.colab import runtime\n",
        "    time.sleep(30)\n",
        "    runtime.unassign()\n",
        "else:\n",
        "    print(\"\\033[96mAuto off est√° desactivado, el entorno no se desconectar√°\\033[0m\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iRvqrBT3HtV"
      },
      "source": [
        "## *Ô∏è‚É£ Extras\n",
        "Puede ejecutarlo antes de comenzar el entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "23_YDYGhEnK0"
      },
      "outputs": [],
      "source": [
        "#@markdown ##Conectarse a Google Drive üìÅ <p> Algunas de las opciones de abajo necesitan conexi√≥n a Google Drive\n",
        "#@markdown conectate aqu√≠.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "print(\"Conexi√≥n establecida a Drive Exitosa!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODpo0kcX3KRy"
      },
      "source": [
        "### üìö Varias carpetas en el mismo conjunto de datos\n",
        "A continuaci√≥n se muestra una plantilla que le permite definir varias carpetas en su conjunto de datos. Debe incluir la ubicaci√≥n de cada carpeta y puede establecer un n√∫mero diferente de repeticiones para cada una. Para agregar m√°s carpetas, simplemente copie y pegue las secciones que comienzan con `[[datasets.subsets]]`.\n",
        "\n",
        "Al habilitar esto, se ignorar√° el n√∫mero de repeticiones establecidas en la celda principal y tambi√©n se ignorar√° la carpeta principal establecida por el nombre del proyecto.\n",
        "\n",
        "Puede convertir uno de ellos en una carpeta de regularizaci√≥n agregando `is_reg = true`  \n",
        "Tambi√©n puede establecer diferentes `keep_tokens`, `flip_aug`, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y037lagnJWmn"
      },
      "outputs": [],
      "source": [
        "custom_dataset = \"\"\"\n",
        "[[datasets]]\n",
        "\n",
        "[[datasets.subsets]]\n",
        "image_dir = \"/content/drive/MyDrive/Loras/example/dataset/good_images\"\n",
        "num_repeats = 3\n",
        "\n",
        "[[datasets.subsets]]\n",
        "image_dir = \"/content/drive/MyDrive/Loras/example/dataset/normal_images\"\n",
        "num_repeats = 1\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W84Jxf-U2TIU"
      },
      "outputs": [],
      "source": [
        "custom_dataset = None"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Extras"
      ],
      "metadata": {
        "id": "y5gg2m3B-daX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ## Colab Activo en el celular üì≥\n",
        "#@markdown Ejecuta esta celda para mantener viva la pesta√±a en el celular (antes de ejecutar la celda de inicio) <p>\n",
        "#@markdown puedes convinarlo con esta [extensi√≥n](https://chromewebstore.google.com/detail/google-colab-keep-alive/bokldcdphgknojlbfhpbbgkggjfhhaek) en tu celular o pc (en celular necesitas un navegador con soporte a extensiones, `firefox` no funcionara)\n",
        "%%html\n",
        "<b>Pulsa play en el reproductor de m√∫sica para mantener viva la pesta√±a antes de ejecutar la celda de inicio (s√≥lo utiliza 13 MB de datos).</b><br/>\n",
        "<audio src=\"https://raw.githubusercontent.com/KoboldAI/KoboldAI-Client/main/colab/silence.m4a\" controls>"
      ],
      "metadata": {
        "id": "ZIL7itnNaw5V",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5ACrHePi3Pen"
      },
      "outputs": [],
      "source": [
        "#@markdown ### üî¢ Contar conjuntos de datos\n",
        "#@markdown Google Drive hace imposible contar los archivos en una carpeta, por lo que le mostrar√° el recuento de archivos en todas las carpetas y subcarpetas.\n",
        "folder = \"/content/drive/MyDrive/Loras/ejemplo/dataset\" #@param {type:\"string\"}\n",
        "\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "if not os.path.exists('/content/drive'):\n",
        "    print(\"üìÇ Connecting to Google Drive...\\n\")\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "tree = {}\n",
        "exclude = (\"_logs\", \"/output\")\n",
        "for i, (root, dirs, files) in enumerate(os.walk(folder, topdown=True)):\n",
        "  dirs[:] = [d for d in dirs if all(ex not in d for ex in exclude)]\n",
        "  images = len([f for f in files if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))])\n",
        "  captions = len([f for f in files if f.lower().endswith(\".txt\")])\n",
        "  others = len(files) - images - captions\n",
        "  path = root[folder.rfind(\"/\")+1:]\n",
        "  tree[path] = None if not images else f\"{images:>4} images | {captions:>4} captions |\"\n",
        "  if tree[path] and others:\n",
        "    tree[path] += f\" {others:>4} other files\"\n",
        "\n",
        "pad = max(len(k) for k in tree)\n",
        "print(\"\\n\".join(f\"üìÅ{k.ljust(pad)} | {v}\" for k, v in tree.items() if v))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ImAtduziVp5h"
      },
      "outputs": [],
      "source": [
        "#@markdown ##Calculador de Repeticiones ‚åõüìù\n",
        "#@markdown Calcula el n√∫mero de repeticiones a usar para entrenar tu lora, Recuerda que en `SDXL y Pony` se usa un batch de `4`.\n",
        "#@markdown Si usas colab pro calcula tus repeticiones con `8` de batch size\n",
        "# Define las Variables\n",
        "# N√∫mero de im√°genes\n",
        "num_images = 40 # @param{type:\"number\"}\n",
        "# N√∫mero de repeticiones\n",
        "num_repeats = 4 # @param{type:\"number\"}\n",
        "# N√∫mero de epocas\n",
        "num_epochs = 10 # @param{type:\"number\"}\n",
        "# Tama√±o de lote\n",
        "batch_size = 4 # @param{type:\"number\"}\n",
        "\n",
        "# Calcula el resultado\n",
        "resultado = (num_images * num_repeats * num_epochs) / batch_size\n",
        "\n",
        "# Muestra el resultado\n",
        "print(\"\\33[96mEl total de repeticiones es:\\033[0m\", resultado)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "1RfwB4UB3M_w"
      },
      "outputs": [],
      "source": [
        "#@markdown ### üìÇ Descomprimir conjunto de datos\n",
        "#@markdown Es mucho m√°s lento cargar archivos individuales en drive, por lo que es posible que desee cargar un archivo zip si tiene su conjunto de datos en su computadra, aqui puedes descomprimirlo.\n",
        "zip = \"/content/drive/MyDrive/my_dataset.zip\" #@param {type:\"string\"}\n",
        "extract_to = \"/content/drive/MyDrive/Loras/example/dataset\" #@param {type:\"string\"}\n",
        "\n",
        "import os, zipfile\n",
        "\n",
        "if not os.path.exists('/content/drive'):\n",
        "  from google.colab import drive\n",
        "  print(\"üìÇ Connecting to Google Drive...\")\n",
        "  drive.mount('/content/drive')\n",
        "\n",
        "os.makedirs(extract_to, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(zip, 'r') as f:\n",
        "  f.extractall(extract_to)\n",
        "\n",
        "print(\"‚úÖ Done\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Iniciar sesi√≥n y crear repositorio en huggingFace ü§ó"
      ],
      "metadata": {
        "id": "R3lcQFnR7bwa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "8YlP0cNQyujy"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "from huggingface_hub import HfApi\n",
        "from huggingface_hub.utils import validate_repo_id, HfHubHTTPError\n",
        "\n",
        "# @markdown crea una cuenta de HuggingFace [aqu√≠](https://huggingface.co/) o si ya la tienes simplemente usa tu token üëá\n",
        "# @markdown > Obten **t√∫** token `EN WRITE` de Huggingface [aqu√≠](https://huggingface.co/settings/tokens) üëà\n",
        "write_token = \"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\"  # @param {type:\"string\"}\n",
        "#@markdown Complete esto si desea cargarlo en su organizaci√≥n, o simplemente d√©jelo vac√≠o.\n",
        "orgs_name = \"\"  # @param{type:\"string\"}\n",
        "# @markdown Si su repositorio de modelo/conjunto de datos no existe, lo crear√° autom√°ticamente.\n",
        "#@markdown No se permiten espacios usa `un gui√≥n bajo` para nombres largos\n",
        "\n",
        "#@markdown El `model_name` se usa para subir archivos individuales\n",
        "model_name = \"\"  # @param{type:\"string\"}\n",
        "#@markdown El dataset se usa para subir carpetas enteras.\n",
        "dataset_name = \"loras\"  # @param{type:\"string\"}\n",
        "#@markdown Marca esta casilla si quieres que tu repositorio/dataset sea privado de lo contrario sera publico\n",
        "make_private = True  # @param{type:\"boolean\"}\n",
        "\n",
        "def authenticate(write_token):\n",
        "    login(write_token, add_to_git_credential=True)\n",
        "    api = HfApi()\n",
        "    return api.whoami(write_token), api\n",
        "\n",
        "\n",
        "def create_repo(api, user, orgs_name, repo_name, repo_type, make_private=False):\n",
        "    global model_repo\n",
        "    global datasets_repo\n",
        "\n",
        "    if orgs_name == \"\":\n",
        "        repo_id = user[\"name\"] + \"/\" + repo_name.strip()\n",
        "    else:\n",
        "        repo_id = orgs_name + \"/\" + repo_name.strip()\n",
        "\n",
        "    try:\n",
        "        validate_repo_id(repo_id)\n",
        "        api.create_repo(repo_id=repo_id, repo_type=repo_type, private=make_private)\n",
        "        print(f\"{repo_type.capitalize()} repo '{repo_id}' didn't exist, creating repo\")\n",
        "    except HfHubHTTPError as e:\n",
        "        print(f\"{repo_type.capitalize()} repo '{repo_id}' exists, skipping create repo\")\n",
        "\n",
        "    if repo_type == \"model\":\n",
        "        model_repo = repo_id\n",
        "        print(f\"{repo_type.capitalize()} repo '{repo_id}' link: https://huggingface.co/{repo_id}\\n\")\n",
        "    else:\n",
        "        datasets_repo = repo_id\n",
        "        print(f\"{repo_type.capitalize()} repo '{repo_id}' link: https://huggingface.co/datasets/{repo_id}\\n\")\n",
        "\n",
        "user, api = authenticate(write_token)\n",
        "\n",
        "if model_name:\n",
        "    create_repo(api, user, orgs_name, model_name, \"model\", make_private)\n",
        "if dataset_name:\n",
        "    create_repo(api, user, orgs_name, dataset_name, \"dataset\", make_private)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Subir a huggingface_hub (Lora y dataset) ü§ó"
      ],
      "metadata": {
        "id": "OFFNDwd87L_4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "EVrt9JJRCxmk"
      },
      "outputs": [],
      "source": [
        "#@markdown ##Subir a trav√©s de huggingface_hub (Lora) ü§ó\n",
        "\n",
        "from huggingface_hub import HfApi\n",
        "from pathlib import Path\n",
        "\n",
        "api = HfApi()\n",
        "file_path = \"/content/drive/MyDrive/Loras/Mi_proyecto/output/Mi_lora.safetensors\" #@param {type :\"string\"}\n",
        "#@markdown Comentario (Opcional) puede ser util para identificar versiones del lora, `ejemplo: pony, animagine, V1, V2 etc...`\n",
        "commit_message = \"\"  #@param {type :\"string\"}\n",
        "\n",
        "if file_path != \"\":\n",
        "  path_obj = Path(file_path)\n",
        "  trained_model = path_obj.parts[-1]\n",
        "\n",
        "  print(f\"Uploading {trained_model} to https://huggingface.co/\"+model_repo)\n",
        "  print(f\"Please wait...\")\n",
        "\n",
        "  api.upload_file(\n",
        "      path_or_fileobj=file_path,\n",
        "      path_in_repo=trained_model,\n",
        "      repo_id=model_repo,\n",
        "      commit_message=commit_message,\n",
        "  )\n",
        "\n",
        "  print(f\"Upload success, located at https://huggingface.co/\"+model_repo+\"/blob/main/\"+trained_model+\"\\n\")\n",
        "print('¬°¬°Subida finalizada!!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "68XMHMLe--If"
      },
      "outputs": [],
      "source": [
        "# @markdown ## Subir carpeta üìÅ a HuggingFace ü§ó\n",
        "from huggingface_hub import HfApi\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "api = HfApi()\n",
        "\n",
        "# @markdown Puedes subir una carpeta entera a Huggingface con tus loras entrenados, incluso puedes subir toda la carpeta de tu proyecto (incluido sub-carpetas), ten paciencia podria tardar un tiempo en cargar varios archivos.\n",
        "folder_path = \"/content/drive/MyDrive/Loras/Mi_lora/output\"  # @param {type :\"string\"}\n",
        "\n",
        "# @markdown Nombre personalizado para la carpeta a subir\n",
        "folder_name = \"Mi_lora\"  # @param {type :\"string\"}\n",
        "\n",
        "# @markdown Comentario (Opcional)\n",
        "commit_message = \"\"  # @param {type :\"string\"}\n",
        "\n",
        "if not commit_message:\n",
        "    commit_message = \"feat: upload folder\"\n",
        "\n",
        "def upload_folder(folder_path, folder_name):\n",
        "    print(f\"Uploading {folder_name} to https://huggingface.co/datasets/{datasets_repo}\")\n",
        "    print(\"Please wait...\")\n",
        "\n",
        "    api.upload_folder(\n",
        "        folder_path=folder_path,\n",
        "        path_in_repo=folder_name,\n",
        "        repo_id=datasets_repo,\n",
        "        repo_type=\"dataset\",\n",
        "        commit_message=commit_message,\n",
        "        ignore_patterns=\".ipynb_checkpoints\",\n",
        "    )\n",
        "    print(f\"Carga terminada, puedes encontrarlo en https://huggingface.co/datasets/{datasets_repo}/tree/main/{folder_name}\\n\")\n",
        "\n",
        "def upload():\n",
        "    if folder_path:\n",
        "        upload_folder(folder_path, folder_name)\n",
        "\n",
        "upload()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7CL4Ckp3SeI"
      },
      "source": [
        "# üìà Gr√°ficas del entrenamiento\n",
        "Puedes hacer esto despu√©s de ejecutar el entrenador.  No necesitas esto a menos que sepas lo que est√°s haciendo.  \n",
        " Es posible que la primera celda a continuaci√≥n no cargue todos sus registros.  Contin√∫e probando la segunda celda hasta que se hayan cargado todos los datos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_TRI3eX90Rp"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir={log_folder}/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rM5SLq990Rp"
      },
      "outputs": [],
      "source": [
        "from tensorboard import notebook\n",
        "notebook.display(port=6006, height=800)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}