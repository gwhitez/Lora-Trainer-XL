{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gwhitez/Lora-Trainer-XL/blob/main/Fix_Lora_Trainer_XL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omCLgUOB2_ZF"
      },
      "source": [
        "# 🌟 XL Lora Trainer\n",
        "\n",
        "❗ **Se recomienda Colab Premium.** Lo ideal sería cambiar el tiempo de ejecución a una A100 y utilizar el tamaño máximo de lote.  \n",
        "Sin embargo, aún se puede entrenar de forma gratuita si carga un modelo de diffusers, solo que tomará mucho más tiempo.  \n",
        "\n",
        "\n",
        "This colab is based on the work of [Kohya-ss](https://github.com/kohya-ss/sd-scripts) and [Linaqruf](https://github.com/Linaqruf/kohya-trainer). Thank you!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJ8clWTZEu-g"
      },
      "source": [
        "### ⭕ Disclaimer\n",
        "The purpose of this document is to research bleeding-edge technologies in the field of machine learning inference.  \n",
        "Please read and follow the [Google Colab guidelines](https://research.google.com/colaboratory/faq.html) and its [Terms of Service](https://research.google.com/colaboratory/tos_v3.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPQlB4djNm3C"
      },
      "source": [
        "| |GitHub|Trainer XL|Tagger WDV2|Tagger WDV3||Old Trainer XL|\n",
        "|:--|:-:|:-:|:-:||:-:|:-:|\n",
        "| 🏠 **Original Proyect** |[![GitHub](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/github.svg)](https://github.com/hollowstrawberry/kohya-colab) | |\n",
        "|**Modified By WhiteZ**| [![GitHub](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/github.svg)](https://github.com/gwhitez/Lora-Trainer-XL) |   [![Open in Colab](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/colab-badge.svg)](https://colab.research.google.com/github/gwhitez/Lora-Trainer-XL/blob/main/Fix_Lora_Trainer_XL.ipynb) |[![Open in Colab](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/colab-badge.svg)](https://colab.research.google.com/github/gwhitez/Lora-Trainer-XL/blob/main/Dataset_Maker_By_WhiteZ.ipynb) | [![Open in Colab](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/colab-badge.svg)](https://colab.research.google.com/github/gwhitez/Lora-Trainer-XL/blob/main/Waifu_Diffusion_V3_Dataser_Maker.ipynb) |  |[![Open in Colab](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/colab-badge.svg)](https://colab.research.google.com/github/gwhitez/Lora-Trainer-XL/blob/main/Old_Fix_Lora_Trainer_XL.ipynb)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "AT1WRbhiHRCJ"
      },
      "outputs": [],
      "source": [
        "import os, re, toml\n",
        "from time import time\n",
        "import time\n",
        "from IPython.display import Markdown, display, HTML, clear_output\n",
        "\n",
        "\n",
        "#auto off\n",
        "#@markdown ### 💤 Auto Off\n",
        "\n",
        "#@markdown *Si esta opción está marcada, el entorno se desconectará automáticamente una vez finalice el entrenamiento.*\n",
        "auto_off   = True  # @param {type: \"boolean\"}\n",
        "if auto_off:\n",
        "    print(\"\\033[96mAuto Off encendido\\033[0m\")\n",
        "if not auto_off:\n",
        "    print(\"\\033[96mAuto Off apagado\\033[0m\")\n",
        "\n",
        "# These carry information from past executions\n",
        "if \"model_url\" in globals():\n",
        "  old_model_url = model_url\n",
        "else:\n",
        "  old_model_url = None\n",
        "if \"dependencies_installed\" not in globals():\n",
        "  dependencies_installed = False\n",
        "if \"model_file\" not in globals():\n",
        "  model_file = None\n",
        "\n",
        "# These may be set by other cells, some are legacy\n",
        "if \"custom_dataset\" not in globals():\n",
        "  custom_dataset = None\n",
        "if \"override_dataset_config_file\" not in globals():\n",
        "  override_dataset_config_file = None\n",
        "if \"continue_from_lora\" not in globals():\n",
        "  continue_from_lora = \"\"\n",
        "if \"override_config_file\" not in globals():\n",
        "  override_config_file = None\n",
        "\n",
        "COMMIT = \"fa2427c6b468231e8e270e40fe72add780118dbe\"\n",
        "LOWRAM = True\n",
        "LOAD_TRUNCATED_IMAGES = True\n",
        "BETTER_EPOCH_NAMES = True\n",
        "FIX_DIFFUSERS = True\n",
        "FIX_WANDB_WARNING = True\n",
        "\n",
        "#@title ## 🚩 Start Here\n",
        "\n",
        "#@markdown ### ▶️ Setup\n",
        "#@markdown El nombre de tu proyecto será el mismo que el de la carpeta que contiene tus imágenes. No se permiten espacios, puedes usar `guión bajo` si el nombre es muy largo.\n",
        "project_name = \"\" #@param {type:\"string\"}\n",
        "project_name = project_name.strip()\n",
        "#@markdown La estructura de carpetas no importa y es puramente por comodidad. Asegúrate de elegir siempre el mismo.  Me gusta organizar por proyecto.\n",
        "folder_structure = \"Organize by project (MyDrive/Loras/project_name/dataset)\" #@param [\"Organize by category (MyDrive/lora_training/datasets/project_name)\", \"Organize by project (MyDrive/Loras/project_name/dataset)\"]\n",
        "#@markdown Decida el modelo que se descargará y utilizará para el entrenamiento. También puedes elegir tu propio modelo pegando su enlace de descarga o un archivo en tu Google Drive que comience con `/content/drive/MyDrive`.\n",
        "training_model = \"Illustrious_2.0\" # @param [\"Pony Diffusion V6 XL\",\"Animagine XL V3\",\"animagine_4.0_zero\",\"Illustrious_0.1\",\"Illustrious_2.0\",\"NoobAI-XL0.75\",\"NoobAI-XL0.5\",\"Stable Diffusion XL 1.0 base\",\"NoobAI-XL-Vpred-v0.75\",\"noobai-xl-vpred-v0.5\"]\n",
        "optional_custom_training_model = \"\" #@param {type:\"string\"}\n",
        "#@markdown Marca esta opción si deseas entrenar con un modelo en formarto diffusers o el modelo custom esta en dicho formato\n",
        "custom_model_is_diffusers = False #@param {type:\"boolean\"}\n",
        "#@markdown Marca esta opción si tu modelo soporta vpred de lo contrario dejala desmarcada.\n",
        "custom_model_is_vpred = False #@param {type:\"boolean\"}\n",
        "#@markdown Utilice wandb si desea visualizar el progreso de su entrenamiento a lo largo del tiempo.\n",
        "wandb_key = \"\" #@param {type:\"string\"}\n",
        "\n",
        "load_diffusers = custom_model_is_diffusers and len(optional_custom_training_model) > 0\n",
        "vpred = custom_model_is_vpred and len(optional_custom_training_model) > 0\n",
        "\n",
        "if optional_custom_training_model:\n",
        "  model_url = optional_custom_training_model\n",
        "elif \"Pony\" in training_model:\n",
        "  if load_diffusers:\n",
        "    model_url = \"https://huggingface.co/WhiteAiZ/Pony_diffusion_v6_diffusers_fp16\"\n",
        "  else:\n",
        "    model_url = \"https://huggingface.co/WhiteAiZ/PonyXL/resolve/main/PonyDiffusionV6XL.safetensors\"\n",
        "  model_file = \"/content/ponyDiffusionV6XL.safetensors\"\n",
        "elif \"Animagine\" in training_model:\n",
        "  if load_diffusers:\n",
        "    model_url = \"https://huggingface.co/cagliostrolab/animagine-xl-3.0\"\n",
        "  else:\n",
        "    model_url = \"https://civitai.com/api/download/models/293564\"\n",
        "  model_file = \"/content/animagineXLV3.safetensors\"\n",
        "elif \"animagine_4.0_zero\" in training_model:\n",
        "  if load_diffusers:\n",
        "    model_url = \"https://huggingface.co/cagliostrolab/animagine-xl-4.0-zero\"\n",
        "  else:\n",
        "    model_url = \"https://huggingface.co/cagliostrolab/animagine-xl-4.0-zero/resolve/main/animagine-xl-4.0-zero.safetensors\"\n",
        "  model_file = \"/content/animagine-xl-4.0-zero.safetensors\"\n",
        "elif \"Illustrious_0.1\" in training_model:\n",
        "  if load_diffusers:\n",
        "    model_url = \"https://huggingface.co/OnomaAIResearch/Illustrious-xl-early-release-v0\"\n",
        "  else:\n",
        "    model_url = \"https://huggingface.co/OnomaAIResearch/Illustrious-xl-early-release-v0/resolve/main/Illustrious-XL-v0.1.safetensors\"\n",
        "elif \"Illustrious_2.0\" in training_model:\n",
        "  if load_diffusers:\n",
        "    model_url = \"https://huggingface.co/WhiteAiZ/Illustrious_2.0\"\n",
        "  else:\n",
        "    model_url = \"https://huggingface.co/WhiteAiZ/Illustrious_2.0/resolve/main/illustriousXL20_v20.safetensors\"\n",
        "  model_file = \"/content/illustriousXL20_v20.safetensors\"\n",
        "elif \"NoobAI-XL0.75\" in training_model:\n",
        "  if load_diffusers:\n",
        "    model_url = \"https://huggingface.co/Laxhar/noobai-XL-0.75\"\n",
        "  else:\n",
        "    model_url = \"https://huggingface.co/Laxhar/noobai-XL-0.75/resolve/main/NoobAI-XL-v0.75.safetensors\"\n",
        "elif \"NoobAI-XL0.5\" in training_model:\n",
        "  if load_diffusers:\n",
        "    model_url = \"https://huggingface.co/Laxhar/noobai-XL-0.5\"\n",
        "  else:\n",
        "    model_url = \"https://huggingface.co/Laxhar/noobai-XL-0.5/resolve/main/NoobAI-XL-v0.5.safetensors\"\n",
        "else:\n",
        "  if load_diffusers:\n",
        "    model_url = \"https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\"\n",
        "  else:\n",
        "    model_url = \"https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/resolve/main/sd_xl_base_1.0.safetensors\"\n",
        "\n",
        "  if load_diffusers:\n",
        "    model_url = \"https://huggingface.co/stablediffusionapi/pony-realism\"\n",
        "  else:\n",
        "    model_url = \"https://huggingface.co/LyliaEngine/ponyRealism_v21MainVAE/resolve/main/ponyRealism_v21MainVAE.safetensors\"\n",
        "  model_file = \"/content/ponyrealism.safetensors\"\n",
        "  vpred = True\n",
        "  if load_diffusers:\n",
        "    model_url = \"https://huggingface.co/Laxhar/noobai-XL-Vpred-0.75\"\n",
        "  else:\n",
        "    model_url = \"https://huggingface.co/Laxhar/noobai-XL-Vpred-0.75/resolve/main/NoobAI-XL-Vpred-v0.75.safetensors\"\n",
        "  model_file = \"/content/NoobAI-XL-Vpred-v0.75.safetensors\"\n",
        "\n",
        "  if load_diffusers:\n",
        "    model_url = \"https://huggingface.co/WhiteAiZ/noobai-xl-vpred-v0.5_diffusers\"\n",
        "  else:\n",
        "    model_url = \"https://huggingface.co/Laxhar/noobai-XL-Vpred-0.5/resolve/main/noobai-xl-vpred-v0.5.safetensors\"\n",
        "  model_file = \"/content/noobai-xl-vpred-v0.5.safetensors\"\n",
        "\n",
        "if load_diffusers:\n",
        "  vae_file= \"stabilityai/sdxl-vae\"\n",
        "else:\n",
        "  vae_url = \"https://huggingface.co/stabilityai/sdxl-vae/resolve/main/sdxl_vae.safetensors\"\n",
        "  vae_file = \"/content/sdxl_vae.safetensors\"\n",
        "\n",
        "model_url = model_url.strip()\n",
        "\n",
        "#@markdown ### ▶️ Processing\n",
        "#@markdown Por defecto la resolución para personajes es 1024. otras resoluciones que puedes usar son 896 (recomendado para personajes o 1024) y 768 (recomendado para estilos, puedes usar más repeticiones con esta resolución).\n",
        "resolution = 768 #@param {type:\"dropdown\", min:768, max:1536, step:128}\n",
        "#@markdown Activa `Flip Aug`si tu dataset es pequeño, util en personajes isometricos, volteara todas tus imagenes (modo espejo) para aprender el doble, pero podria afectar a personajes con tatuajes, marcas, cicatrices etc...\n",
        "flip_aug = False #@param {type:\"boolean\"}\n",
        "caption_extension = \".txt\" # @param [\".txt\",\".caption\"]\n",
        "#@markdown Mezcla etiquetas de anime, mejora el aprendizaje y las indicaciones.  Una etiqueta de activación va al comienzo de cada archivo de texto y no se mezclará.<p>\n",
        "shuffle_tags = False #@param {type:\"boolean\"}\n",
        "shuffle_caption = shuffle_tags\n",
        "activation_tags = \"0\" #@param [0,1,2,3]\n",
        "keep_tokens = int(activation_tags)\n",
        "\n",
        "#@markdown ### ▶️ Steps <p>\n",
        "#@markdown Tus imágenes se repetirán esta cantidad de veces durante el entrenamiento. Te recomiendo que tus imágenes multiplicadas por sus repeticiones esté entre 200 y 400.\n",
        "num_repeats = 2 #@param {type:\"number\"}\n",
        "#@markdown Elige cuánto tiempo quieres entrenar.  Un buen punto de partida es alrededor de 10 épocas o alrededor de 2000 pasos.<p>\n",
        "#@markdown Una época es una cantidad de pasos igual a: la cantidad de imágenes multiplicada por sus repeticiones, dividida por el tamaño del lote. <p>\n",
        "preferred_unit = \"Epochs\" #@param [\"Epochs\", \"Steps\"]\n",
        "how_many = 15 #@param {type:\"number\"}\n",
        "max_train_epochs = how_many if preferred_unit == \"Epochs\" else None\n",
        "max_train_steps = how_many if preferred_unit == \"Steps\" else None\n",
        "#@markdown Guardar más épocas te permitirá comparar mejor el progreso de tu Lora.\n",
        "save_every_n_epochs = 1 #@param {type:\"number\"}\n",
        "keep_only_last_n_epochs = 5 #@param {type:\"number\"}\n",
        "if not save_every_n_epochs:\n",
        "  save_every_n_epochs = max_train_epochs\n",
        "if not keep_only_last_n_epochs:\n",
        "  keep_only_last_n_epochs = max_train_epochs\n",
        "\n",
        "#@markdown ### ▶️ Learning\n",
        "#@markdown La tasa de aprendizaje es lo más importante para tus resultados. Si quieres entrenar más lento con muchas imágenes, o si tu dim y alfa son altos, mueve el unet a 2e-4 o menos.  <p>\n",
        "#@markdown El codificador de texto ayuda al Lora a aprender conceptos un poco mejor.  Se recomienda hacerlo la mitad o una quinta parte del unet.  Si estás entrenando un estilo, puedes incluso configurarlo en 0.\n",
        "unet_lr = 2e-5 #@param {type:\"number\"}\n",
        "text_encoder_lr = 0 #@param {type:\"number\"}\n",
        "#@markdown El scheduler es el algoritmo que guía la tasa de aprendizaje. Si no está seguro, elije \"constant\" e ignore el número. Personalmente recomiendo `cosine_with_restarts` con 3 reinicios.\n",
        "lr_scheduler = \"rex\" # @param [\"constant\",\"cosine\",\"cosine_with_restarts\",\"constant_with_warmup\",\"linear\",\"polynomial\",\"rex\"]\n",
        "lr_scheduler_number = 1 #@param {type:\"number\"}\n",
        "#@markdown Pasos dedicados a \"calentar\" la tasa de aprendizaje durante la capacitación para lograr eficiencia. Recomiendo dejarlo al 5%.\n",
        "lr_warmup_ratio = 0.05 #@param {type:\"slider\", min:0.0, max:0.2, step:0.01}\n",
        "lr_warmup_steps = 0 #@param {type:\"number\"}\n",
        "#@markdown Estas configuraciones pueden producir mejores resultados.`min_snr_gamma` ajusta la pérdida con el tiempo. `ip_noise_gamma` ajusta el ruido aleatorio.\n",
        "min_snr_gamma_enabled = True #@param {type:\"boolean\"}\n",
        "min_snr_gamma = 8.0 #@param {type:\"slider\", min:4, max:16.0, step:0.5}\n",
        "ip_noise_gamma_enabled = True #@param {type:\"boolean\"}\n",
        "ip_noise_gamma = 0.05 #@param {type:\"slider\", min:0.05, max:0.1, step:0.01}\n",
        "#@markdown Multinoise puede ayudar con el equilibrio del color (negros más oscuros, blancos más claros) no es necesario activarlo si entrenas Lora Vpred.\n",
        "multinoise = True #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ### ▶️ Structure\n",
        "#@markdown LoRA es del tipo clásico y bueno para una variedad de propósitos. LoCon es bueno con los estilos artísticos (también funciona con personajes) ya que tiene más capas para aprender más aspectos del conjunto de datos.\n",
        "lora_type = \"LoRA\" # @param [\"LoRA\",\"LoCon\"]\n",
        "\n",
        "#@markdown A continuación se muestran algunos valores XL recomendados para las siguientes configuraciones:\n",
        "\n",
        "#@markdown | type | network_dim | network_alpha | conv_dim | conv_alpha |\n",
        "#@markdown | :---: | :---: | :---: | :---: | :---: |\n",
        "#@markdown | Personaje LoRA | 4 | 16 |   |   |\n",
        "#@markdown | Regular y Estilo LoRA | 8 | 4 |   |   |\n",
        "#@markdown | Style LoCon | 16 | 8 | 16 | 8 |\n",
        "\n",
        "#@markdown Más dim significa un Lora más grande, puede contener más información, pero más no siempre es mejor.\n",
        "network_dim = 8 #@param {type:\"number\", min:1, max:32, step:1}\n",
        "network_alpha = 16 #@param {type:\"number\", min:1, max:32, step:1}\n",
        "#@markdown Los siguientes dos valores solo se aplican a las capas adicionales de LoCon.\n",
        "conv_dim = 16 #@param {type:\"number\", min:1, max:32, step:1}\n",
        "conv_alpha = 8 #@param {type:\"number\", min:1, max:32, step:1}\n",
        "\n",
        "network_module = \"networks.lora\"\n",
        "network_args = None\n",
        "if lora_type.lower() == \"locon\":\n",
        "  network_args = [f\"conv_dim={conv_dim}\", f\"conv_alpha={conv_alpha}\"]\n",
        "\n",
        "#@markdown ### ▶️ Training\n",
        "#@markdown Ajuste estos parámetros según la configuración de su colab.\n",
        "\n",
        "#@markdown El batch size de 4 es el predeterminado pero puedes incrementarlo incluso a 8 usando una resolución baja (768).\n",
        "#@markdown\n",
        "#@markdown Un tamaño de lote más alto suele ser más rápido pero utiliza más memoria.\n",
        "train_batch_size = 4 #@param {type:\"slider\", min:1, max:16, step:1}\n",
        "#@markdown xformers funciona mejor que sdpa con los nuevos scrips.\n",
        "cross_attention = \"xformers\" #@param [\"sdpa\", \"xformers\"]\n",
        "#@markdown Utilice `full fp16` para el uso mínimo de memoria. <p>\n",
        "#@markdown `float, full bf16, full fp16, mixed bf16 y mixed fp16` solo funcionaran con colab pro. <p>\n",
        "#@markdown El Lora se entrenará con la precisión seleccionada, pero siempre se guardará en formato fp16 por razones de compatibilidad.\n",
        "precision = \"full fp16\" #@param [\"float\", \"full fp16\", \"full bf16\", \"mixed fp16\", \"mixed bf16\"]\n",
        "#@markdown El almacenamiento en caché latente en drive agregará un archivo de 250 KB junto a cada imagen, pero usará considerablemente menos memoria.\n",
        "cache_latents = True #@param {type:\"boolean\"}\n",
        "cache_latents_to_drive = False #@param {type:\"boolean\"}\n",
        "#@markdown La siguiente opción desactivará shuffle_tags y deshabilitará el entrenamiento del codificador de texto.\n",
        "cache_text_encoder_outputs  = False  # @param {type:\"boolean\"}\n",
        "\n",
        "mixed_precision = \"no\"\n",
        "if \"fp16\" in precision:\n",
        "  mixed_precision = \"fp16\"\n",
        "elif \"bf16\" in precision:\n",
        "  mixed_precision = \"bf16\"\n",
        "full_precision = \"full\" in precision\n",
        "\n",
        "#@markdown ### ▶️ Advanced\n",
        "#@markdown El optimizador es el algoritmo utilizado para el entrenamiento. Adafactor es el predeterminado y funciona muy bien, mientras que el Prodigy administra la tasa de aprendizaje automáticamente y puede tener varias ventajas, como entrenar más rápido, debido a que necesita menos pasos y funcionan mejor para datasets pequeños.\n",
        "optimizer = \"Came\" #@param [\"AdamW8bit\", \"Prodigy\", \"DAdaptation\", \"DadaptAdam\", \"DadaptLion\", \"AdamW\", \"Lion\", \"SGDNesterov\", \"SGDNesterov8bit\", \"AdaFactor\", \"Came\"]\n",
        "#@markdown Argumentos recomendados para Adafactor: `scale_parameter=False relative_step=False warmup_init=False` <p>\n",
        "#@markdown Argumentos recomendados para AdamW8bit: `weight_decay=0.1 betas=[0.9,0.99]`<p>\n",
        "#@markdown Argumentos recomendados para Prodigy: `decouple=True weight_decay=0.01 betas=[0.9,0.999] d_coef=2 use_bias_correction=True safeguard_warmup=True`<p>\n",
        "#@markdown Argumentos recomendado para CAME: `weight_decay=0.04` <p>\n",
        "#@markdown Si se selecciona Dadapt o Prodigy y se marca la casilla recomendada, los siguientes valores recomendados anularán cualquier configuración anterior:<p>\n",
        "#@markdown `unet_lr=0.75`, `text_encoder_lr=0.75`, `network_alpha=network_dim`, `full_precision=True`<p>\n",
        "#@markdown Si selecciona Prodigy o Dadapt recomiendo usar `mixed fp16`para mejores resultados. <p>\n",
        "recommended_values = True #@param {type:\"boolean\"}\n",
        "#@markdown Alternativamente, establezca sus propios argumentos de optimizador separados por espacios (no comas). `recommended_values` debe estar deshabilitado.\n",
        "optimizer_args = \"weight_decay=0.04\" #@param {type:\"string\"}\n",
        "optimizer_args = [a.strip() for a in optimizer_args.split(' ') if a]\n",
        "\n",
        "\n",
        "if recommended_values:\n",
        "  if any(opt in optimizer.lower() for opt in [\"dadapt\", \"prodigy\"]):\n",
        "    unet_lr = 0.75\n",
        "    text_encoder_lr = 0.75\n",
        "    network_alpha = network_dim\n",
        "    full_precision = True\n",
        "  if optimizer == \"Prodigy\":\n",
        "    optimizer_args = [\"decouple=True\", \"weight_decay=0.01\", \"betas=[0.9,0.999]\", \"d_coef=2\", \"use_bias_correction=True\", \"safeguard_warmup=True\"]\n",
        "  elif optimizer == \"AdamW8bit\":\n",
        "    optimizer_args = [\"weight_decay=0.1\", \"betas=[0.9,0.99]\"]\n",
        "  elif optimizer == \"AdaFactor\":\n",
        "    optimizer_args = [\"scale_parameter=False\", \"relative_step=False\", \"warmup_init=False\"]\n",
        "  elif optimizer == \"Came\":\n",
        "    optimizer_args = [\"weight_decay=0.04\"]\n",
        "\n",
        "if optimizer == \"Came\":\n",
        "  optimizer = \"LoraEasyCustomOptimizer.came.CAME\"\n",
        "\n",
        "lr_scheduler_type = None\n",
        "lr_scheduler_args = None\n",
        "lr_scheduler_num_cycles = lr_scheduler_number\n",
        "lr_scheduler_power = lr_scheduler_number\n",
        "\n",
        "if \"rex\" in lr_scheduler:\n",
        "  lr_scheduler = \"cosine\"\n",
        "  lr_scheduler_type = \"LoraEasyCustomOptimizer.RexAnnealingWarmRestarts.RexAnnealingWarmRestarts\"\n",
        "  lr_scheduler_args = [\"min_lr=1e-9\", \"gamma=0.9\", \"d=0.9\"]\n",
        "\n",
        "# Misc\n",
        "seed = 42\n",
        "gradient_accumulation_steps = 1\n",
        "bucket_reso_steps = 64\n",
        "min_bucket_reso = 256\n",
        "max_bucket_reso = 4096\n",
        "\n",
        "#@markdown ### ▶️ Ready\n",
        "#@markdown Ahora puedes ejecutar esta celda para entrenar tu Lora. ¡Buena suerte! <p>\n",
        "\n",
        "# 👩‍💻 Cool code goes here\n",
        "\n",
        "root_dir = \"/content\"\n",
        "trainer_dir = os.path.join(root_dir, \"trainer\")\n",
        "kohya_dir = os.path.join(trainer_dir, \"sd_scripts\")\n",
        "\n",
        "venv_python = os.path.join(kohya_dir, \"venv/bin/python\")\n",
        "#venv_pip = os.path.join(kohya_dir, \"venv/bin/pip\")\n",
        "train_network = os.path.join(kohya_dir, \"sdxl_train_network.py\")\n",
        "\n",
        "if \"/Loras\" in folder_structure:\n",
        "  main_dir      = os.path.join(root_dir, \"drive/MyDrive/Loras\")\n",
        "  log_folder    = os.path.join(main_dir, \"_logs\")\n",
        "  config_folder = os.path.join(main_dir, project_name)\n",
        "  images_folder = os.path.join(main_dir, project_name, \"dataset\")\n",
        "  output_folder = os.path.join(main_dir, project_name, \"output\")\n",
        "else:\n",
        "  main_dir      = os.path.join(root_dir, \"drive/MyDrive/lora_training\")\n",
        "  images_folder = os.path.join(main_dir, \"datasets\", project_name)\n",
        "  output_folder = os.path.join(main_dir, \"output\", project_name)\n",
        "  config_folder = os.path.join(main_dir, \"config\", project_name)\n",
        "  log_folder    = os.path.join(main_dir, \"log\")\n",
        "\n",
        "config_file = os.path.join(config_folder, \"training_config.toml\")\n",
        "dataset_config_file = os.path.join(config_folder, \"dataset_config.toml\")\n",
        "\n",
        "def install_trainer():\n",
        "  if 'installed' not in globals():\n",
        "    !wget https://github.com/camenduru/gperftools/releases/download/v1.0/libtcmalloc_minimal.so.4 -O /content/libtcmalloc_minimal.so.4\n",
        "    installed = True\n",
        "\n",
        "  #Uv package manager add-on ZeroProtecPlus\n",
        "  !git clone -b dev https://github.com/ZeroProtecPlus/LoRA_Easy_Training_scripts_Backend {trainer_dir}\n",
        "  os.chdir(trainer_dir)\n",
        "  #!git reset --hard {COMMIT}\n",
        "\n",
        "  display(HTML(\"<h2 style='color: yellow;'>Descargando dependencias</h2>\"))\n",
        "  !chmod 755 ./colab_install.sh\n",
        "  !./colab_install.sh> install_log.txt 2>&1\n",
        "  #clear_output()\n",
        "\n",
        "  # patch kohya\n",
        "  os.chdir(kohya_dir)\n",
        "  if LOAD_TRUNCATED_IMAGES:\n",
        "    !sed -i 's/from PIL import Image/from PIL import Image, ImageFile\\nImageFile.LOAD_TRUNCATED_IMAGES=True/g' library/train_util.py # fix truncated jpegs error\n",
        "  if BETTER_EPOCH_NAMES:\n",
        "    !sed -i 's/{:06d}/{:02d}/g' library/train_util.py # make epoch names shorter\n",
        "    !sed -i 's/\".\" + args.save_model_as)/\"-{:02d}.\".format(num_train_epochs) + args.save_model_as)/g' train_network.py # name of the last epoch will match the rest\n",
        "  if FIX_DIFFUSERS:\n",
        "    deprecation_utils = os.path.join(kohya_dir, \"venv/lib/python3.10/site-packages/diffusers/utils/deprecation_utils.py\")\n",
        "    !sed -i 's/if version.parse/if False:#/g' {deprecation_utils}\n",
        "  if FIX_WANDB_WARNING:\n",
        "    !sed -i 's/accelerator.log(logs, step=epoch + 1)//g' train_network.py\n",
        "    !sed -i 's/accelerator.log(logs, step=epoch + 1)//g' sdxl_train.py\n",
        "\n",
        "  os.environ[\"LD_PRELOAD\"] = \"/content/libtcmalloc_minimal.so.4\"\n",
        "  os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "  os.environ[\"BITSANDBYTES_NOWELCOME\"] = \"1\"\n",
        "  os.environ[\"SAFETENSORS_FAST_GPU\"] = \"1\"\n",
        "  os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n",
        "  os.chdir(root_dir)\n",
        "\n",
        "\n",
        "def validate_dataset():\n",
        "  global lr_warmup_steps, lr_warmup_ratio, caption_extension, keep_tokens, model_url\n",
        "  supported_types = (\".png\", \".jpg\", \".jpeg\", \".webp\", \".bmp\")\n",
        "\n",
        "  if model_url.startswith(\"/content/drive/\") and not os.path.exists(model_url):\n",
        "    print(\"💥 Error: El modelo de entrenamiento personalizado que especificó no se encontró en su Google Drive.\")\n",
        "    return\n",
        "\n",
        "  print(\"\\n💿 Checking dataset...\")\n",
        "  if not project_name.strip() or any(c in project_name for c in \" .()\\\"'\\\\/\"):\n",
        "    print(\"💥 Error: Elija un nombre de proyecto válido.\")\n",
        "    return\n",
        "\n",
        "  # Find the folders and files\n",
        "  if custom_dataset:\n",
        "    try:\n",
        "      datconf = toml.loads(custom_dataset)\n",
        "      datasets = [d for d in datconf[\"datasets\"][0][\"subsets\"]]\n",
        "    except:\n",
        "      print(f\"💥 Error: El conjunto de datos personalizado no es válido o contiene un error. Por favor, compruebe la plantilla original.\")\n",
        "      return\n",
        "    reg = [d.get(\"image_dir\") for d in datasets if d.get(\"is_reg\", False)]\n",
        "    datasets_dict = {d[\"image_dir\"]: d[\"num_repeats\"] for d in datasets}\n",
        "    folders = datasets_dict.keys()\n",
        "    files = [f for folder in folders for f in os.listdir(folder)]\n",
        "    images_repeats = {folder: (len([f for f in os.listdir(folder) if f.lower().endswith(supported_types)]), datasets_dict[folder]) for folder in folders}\n",
        "  else:\n",
        "    reg = []\n",
        "    folders = [images_folder]\n",
        "    files = os.listdir(images_folder)\n",
        "    images_repeats = {images_folder: (len([f for f in files if f.lower().endswith(supported_types)]), num_repeats)}\n",
        "\n",
        "  # Validation\n",
        "  for folder in folders:\n",
        "    if not os.path.exists(folder):\n",
        "      print(f\"💥 Error: La carpeta {folder.replace('/content/drive/', '')} no existe.\")\n",
        "      return\n",
        "  for folder, (img, rep) in images_repeats.items():\n",
        "    if not img:\n",
        "      print(f\"💥 Error: tú {folder.replace('/content/drive/', '')} La carpeta está vacía.\")\n",
        "      return\n",
        "  test_files = []\n",
        "  for f in files:\n",
        "    if not f.lower().endswith((caption_extension, \".npz\")) and not f.lower().endswith(supported_types):\n",
        "      print(f\"💥 Error: Archivo no válido en el conjunto de datos: \\\"{f}\\\". Abortar.\")\n",
        "      return\n",
        "    for ff in test_files:\n",
        "      if f.endswith(supported_types) and ff.endswith(supported_types) \\\n",
        "          and os.path.splitext(f)[0] == os.path.splitext(ff)[0]:\n",
        "        print(f\"💥 Error: Los archivos {f} y {ff} no puede tener el mismo nombre. Abortar.\")\n",
        "        return\n",
        "    test_files.append(f)\n",
        "\n",
        "  if caption_extension and not [txt for txt in files if txt.lower().endswith(caption_extension)]:\n",
        "    caption_extension = \"\"\n",
        "  if continue_from_lora and not (continue_from_lora.endswith(\".safetensors\") and os.path.exists(continue_from_lora)):\n",
        "    print(f\"💥 Error: ruta no válida del Lora existente. Ejemplo: /content/drive/MyDrive/Loras/example.safetensors\")\n",
        "    return\n",
        "\n",
        "  # Show estimations to the user\n",
        "\n",
        "  pre_steps_per_epoch = sum(img*rep for (img, rep) in images_repeats.values())\n",
        "  steps_per_epoch = pre_steps_per_epoch/train_batch_size\n",
        "  total_steps = max_train_steps or int(max_train_epochs*steps_per_epoch)\n",
        "  estimated_epochs = int(total_steps/steps_per_epoch)\n",
        "  lr_warmup_steps = int(total_steps*lr_warmup_ratio)\n",
        "\n",
        "  for folder, (img, rep) in images_repeats.items():\n",
        "    print(\"📁\"+folder.replace(\"/content/drive/\", \"\") + (\" (Regularization)\" if folder in reg else \"\"))\n",
        "    print(f\"📈 Se encontró {img} imágenes con {rep} repeticiones, igual {img*rep} pasos.\")\n",
        "  print(f\"📉 Divide {pre_steps_per_epoch} pasos por {train_batch_size} batch size para obtener {steps_per_epoch} pasos por epoch.\")\n",
        "  if max_train_epochs:\n",
        "    print(f\"🔮 Habrá {max_train_epochs} epochs, por alrededor de {total_steps} total de pasos.\")\n",
        "  else:\n",
        "    print(f\"🔮 Habrá {total_steps} pasos, divididos en {estimated_epochs} epochs y algo más.\")\n",
        "\n",
        "  if total_steps > 10000:\n",
        "    print(\"💥 Error: El total de pasos es demasiado alto. Probablemente cometiste un error. Abortar...\")\n",
        "    return\n",
        "\n",
        "  return True\n",
        "\n",
        "def create_config():\n",
        "  global dataset_config_file, config_file, model_file\n",
        "\n",
        "  if override_config_file:\n",
        "    config_file = override_config_file\n",
        "    print(f\"\\n⭕ Using custom config file {config_file}\")\n",
        "  else:\n",
        "    config_dict = {\n",
        "      \"network_arguments\": {\n",
        "        \"unet_lr\": unet_lr,\n",
        "        \"text_encoder_lr\": text_encoder_lr if not cache_text_encoder_outputs else 0,\n",
        "        \"network_dim\": network_dim,\n",
        "        \"network_alpha\": network_alpha,\n",
        "        \"network_module\": network_module,\n",
        "        \"network_args\": network_args,\n",
        "        \"network_train_unet_only\": text_encoder_lr == 0 or cache_text_encoder_outputs,\n",
        "        \"network_weights\": continue_from_lora or None\n",
        "      },\n",
        "      \"optimizer_arguments\": {\n",
        "        \"learning_rate\": unet_lr,\n",
        "        \"lr_scheduler\": lr_scheduler,\n",
        "        \"lr_scheduler_type\": lr_scheduler_type,\n",
        "        \"lr_scheduler_args\": lr_scheduler_args,\n",
        "        \"lr_scheduler_num_cycles\": lr_scheduler_num_cycles if lr_scheduler == \"cosine_with_restarts\" else None,\n",
        "        \"lr_scheduler_power\": lr_scheduler_power if lr_scheduler == \"polynomial\" else None,\n",
        "        \"lr_warmup_steps\": lr_warmup_steps if lr_scheduler not in (\"cosine\", \"constant\") else None,\n",
        "        \"optimizer_type\": optimizer,\n",
        "       \"optimizer_args\": optimizer_args or None,\n",
        "        \"loss_type\": \"l2\",\n",
        "        \"max_grad_norm\": 1.0,\n",
        "      },\n",
        "      \"training_arguments\": {\n",
        "        \"lowram\": LOWRAM,\n",
        "        \"pretrained_model_name_or_path\": model_file,\n",
        "        \"vae\": vae_file,\n",
        "        \"max_train_steps\": max_train_steps,\n",
        "        \"max_train_epochs\": max_train_epochs,\n",
        "        \"train_batch_size\": train_batch_size,\n",
        "        \"seed\": seed,\n",
        "        \"max_token_length\": 225,\n",
        "        \"xformers\": cross_attention == \"xformers\",\n",
        "        \"sdpa\": cross_attention == \"sdpa\",\n",
        "        \"min_snr_gamma\": min_snr_gamma if min_snr_gamma_enabled else None,\n",
        "        \"ip_noise_gamma\": ip_noise_gamma if ip_noise_gamma_enabled else None,\n",
        "        \"no_half_vae\": True,\n",
        "        \"gradient_checkpointing\": True,\n",
        "        \"gradient_accumulation_steps\": gradient_accumulation_steps,\n",
        "        \"max_data_loader_n_workers\": 1,\n",
        "        \"persistent_data_loader_workers\": True,\n",
        "        \"mixed_precision\": mixed_precision,\n",
        "        \"full_fp16\": mixed_precision == \"fp16\" and full_precision,\n",
        "        \"full_bf16\": mixed_precision == \"bf16\" and full_precision,\n",
        "        \"cache_latents\": cache_latents,\n",
        "        \"cache_latents_to_disk\": cache_latents_to_drive,\n",
        "        \"cache_text_encoder_outputs\": cache_text_encoder_outputs,\n",
        "        \"min_timestep\": 0,\n",
        "        \"max_timestep\": 1000,\n",
        "        \"prior_loss_weight\": 1.0,\n",
        "        \"multires_noise_iterations\": 6 if multinoise else None,\n",
        "        \"multires_noise_discount\": 0.3 if multinoise else None,\n",
        "        \"v_parameterization\": vpred or None,\n",
        "        \"scale_v_pred_loss_like_noise_pred\": vpred or None,\n",
        "        \"zero_terminal_snr\": vpred or None,\n",
        "      },\n",
        "      \"saving_arguments\": {\n",
        "        \"save_precision\": \"fp16\",\n",
        "        \"save_model_as\": \"safetensors\",\n",
        "        \"save_every_n_epochs\": save_every_n_epochs,\n",
        "        \"save_last_n_epochs\": keep_only_last_n_epochs,\n",
        "        \"output_name\": project_name,\n",
        "        \"output_dir\": output_folder,\n",
        "        \"log_prefix\": project_name,\n",
        "        \"logging_dir\": log_folder,\n",
        "        \"wandb_api_key\": wandb_key or None,\n",
        "        \"log_with\": \"wandb\" if wandb_key else None,\n",
        "      }\n",
        "    }\n",
        "\n",
        "    for key in config_dict:\n",
        "      if isinstance(config_dict[key], dict):\n",
        "        config_dict[key] = {k: v for k, v in config_dict[key].items() if v is not None}\n",
        "\n",
        "    with open(config_file, \"w\") as f:\n",
        "      f.write(toml.dumps(config_dict))\n",
        "    print(f\"\\n📄 Config saved to {config_file}\")\n",
        "\n",
        "  if override_dataset_config_file:\n",
        "    dataset_config_file = override_dataset_config_file\n",
        "    print(f\"⭕ Using custom dataset config file {dataset_config_file}\")\n",
        "  else:\n",
        "    dataset_config_dict = {\n",
        "      \"general\": {\n",
        "        \"resolution\": resolution,\n",
        "        \"shuffle_caption\": shuffle_caption and not cache_text_encoder_outputs,\n",
        "        \"keep_tokens\": keep_tokens,\n",
        "        \"flip_aug\": False,\n",
        "        \"caption_extension\": caption_extension,\n",
        "        \"enable_bucket\": True,\n",
        "        \"bucket_no_upscale\": False,\n",
        "        \"bucket_reso_steps\": bucket_reso_steps,\n",
        "        \"min_bucket_reso\": min_bucket_reso,\n",
        "        \"max_bucket_reso\": max_bucket_reso,\n",
        "      },\n",
        "      \"datasets\": toml.loads(custom_dataset)[\"datasets\"] if custom_dataset else [\n",
        "        {\n",
        "          \"subsets\": [\n",
        "            {\n",
        "              \"num_repeats\": num_repeats,\n",
        "              \"image_dir\": images_folder,\n",
        "              \"class_tokens\": None if caption_extension else project_name\n",
        "            }\n",
        "          ]\n",
        "        }\n",
        "      ]\n",
        "    }\n",
        "\n",
        "    for key in dataset_config_dict:\n",
        "      if isinstance(dataset_config_dict[key], dict):\n",
        "        dataset_config_dict[key] = {k: v for k, v in dataset_config_dict[key].items() if v is not None}\n",
        "\n",
        "    with open(dataset_config_file, \"w\") as f:\n",
        "      f.write(toml.dumps(dataset_config_dict))\n",
        "    print(f\"📄 Configuración de dataset guardada en {dataset_config_file}\")\n",
        "\n",
        "def download_model():\n",
        "  global old_model_url, model_url, model_file, vae_url, vae_file\n",
        "  real_model_url = model_url  # There was a reason for having a separate variable but I forgot what it was.\n",
        "\n",
        "  if real_model_url.startswith(\"/content/drive/\"):\n",
        "    # Local model, already checked to exist\n",
        "    model_file = real_model_url\n",
        "    print(f\"📁 Using local model file: {model_file}\")\n",
        "\n",
        "        # Download VAE\n",
        "    if not os.path.exists(vae_file):\n",
        "      !aria2c \"{vae_url}\" --console-log-level=warn -c -s 16 -x 16 -k 10M -d / -o \"{vae_file}\"\n",
        "\n",
        "    # Validation\n",
        "    if model_file.lower().endswith(\".safetensors\"):\n",
        "      from safetensors.torch import load_file as load_safetensors\n",
        "      try:\n",
        "        test = load_safetensors(model_file)\n",
        "        del test\n",
        "      except:\n",
        "        return False\n",
        "    elif model_file.lower().endswith(\".ckpt\"):\n",
        "      from torch import load as load_ckpt\n",
        "      try:\n",
        "        test = load_ckpt(model_file)\n",
        "        del test\n",
        "      except:\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "  else:\n",
        "    # Downloadable model\n",
        "    if load_diffusers:\n",
        "      if 'huggingface.co' in real_model_url:\n",
        "          match = re.search(r'huggingface\\.co/([^/]+)/([^/]+)', real_model_url)\n",
        "          if match:\n",
        "              username = match.group(1)\n",
        "              model_name = match.group(2)\n",
        "              model_file = f\"{username}/{model_name}\"\n",
        "              from huggingface_hub import HfFileSystem\n",
        "              fs = HfFileSystem()\n",
        "              existing_folders = set(fs.ls(model_file, detail=False))\n",
        "              necessary_folders = [ \"scheduler\", \"text_encoder\", \"text_encoder_2\", \"tokenizer\", \"tokenizer_2\", \"unet\", \"vae\" ]\n",
        "              if all(f\"{model_file}/{folder}\" in existing_folders for folder in necessary_folders):\n",
        "                print(\"🍃 Modelo diffusers identificado.\")  # Will be handled by kohya\n",
        "                return True\n",
        "      raise ValueError(\"💥 Failed to load Diffusers model. If this model is not Diffusers, have you tried turning it off at the top of the colab?\")\n",
        "\n",
        "    # Define local filename\n",
        "    if not model_file or old_model_url and old_model_url != model_url:\n",
        "      if real_model_url.lower().endswith((\".ckpt\", \".safetensors\")):\n",
        "        model_file = f\"/content{real_model_url[real_model_url.rfind('/'):]}\"\n",
        "      else:\n",
        "        model_file = \"/content/downloaded_model.safetensors\"\n",
        "        if os.path.exists(model_file):\n",
        "          !rm \"{model_file}\"\n",
        "\n",
        "    # HuggingFace\n",
        "    if m := re.search(r\"(?:https?://)?(?:www\\.)?huggingface\\.co/[^/]+/[^/]+/blob\", real_model_url):\n",
        "      real_model_url = real_model_url.replace(\"blob\", \"resolve\")\n",
        "    # Civitai\n",
        "    elif m := re.search(r\"(?:https?://)?(?:www\\\\.)?civitai\\.com/models/([0-9]+)(/[A-Za-z0-9-_]+)?\", real_model_url):\n",
        "      if m.group(2):\n",
        "        model_file = f\"/content{m.group(2)}.safetensors\"\n",
        "      if m := re.search(r\"modelVersionId=([0-9]+)\", real_model_url):\n",
        "        real_model_url = f\"https://civitai.com/api/download/models/{m.group(1)}\"\n",
        "      else:\n",
        "        raise ValueError(\"💥 optional_custom_training_model contains a civitai link, but the link doesn't include a modelVersionId. You can also right click the download button to copy the direct download link.\")\n",
        "\n",
        "    # Download checkpoint\n",
        "    !aria2c \"{real_model_url}\" --console-log-level=warn -c -s 16 -x 16 -k 10M -d / -o \"{model_file}\"\n",
        "\n",
        "    # Download VAE\n",
        "    if not os.path.exists(vae_file):\n",
        "      !aria2c \"{vae_url}\" --console-log-level=warn -c -s 16 -x 16 -k 10M -d / -o \"{vae_file}\"\n",
        "\n",
        "    # Validation\n",
        "\n",
        "    if model_file.lower().endswith(\".safetensors\"):\n",
        "      from safetensors.torch import load_file as load_safetensors\n",
        "      try:\n",
        "        test = load_safetensors(model_file)\n",
        "        del test\n",
        "      except:\n",
        "        new_model_file = os.path.splitext(model_file)[0]+\".ckpt\"\n",
        "        !mv \"{model_file}\" \"{new_model_file}\"\n",
        "        model_file = new_model_file\n",
        "        print(f\"Renamed model to {os.path.splitext(model_file)[0]}.ckpt\")\n",
        "\n",
        "    if model_file.lower().endswith(\".ckpt\"):\n",
        "      from torch import load as load_ckpt\n",
        "      try:\n",
        "        test = load_ckpt(model_file)\n",
        "        del test\n",
        "      except:\n",
        "        return False\n",
        "\n",
        "  return True\n",
        "\n",
        "\n",
        "def calculate_rex_steps():\n",
        "  # https://github.com/derrian-distro/LoRA_Easy_Training_scripts_Backend/blob/c34084b0435e6e19bb7a01ac1ecbadd185ee8c1e/utils/validation.py#L268\n",
        "  global max_train_steps\n",
        "  print(\"\\n🤔 Calculating Rex steps\")\n",
        "  if max_train_steps:\n",
        "    calculated_max_steps = max_train_steps\n",
        "  else:\n",
        "    from library.train_util import BucketManager\n",
        "    from PIL import Image\n",
        "    from pathlib import Path\n",
        "    import math\n",
        "\n",
        "    with open(dataset_config_file, \"r\") as f:\n",
        "      subsets = toml.load(f)[\"datasets\"][0][\"subsets\"]\n",
        "\n",
        "    supported_types = [\".png\", \".jpg\", \".jpeg\", \".webp\", \".bmp\"]\n",
        "    res = (resolution, resolution)\n",
        "    bucketManager = BucketManager(False, res, min_bucket_reso, max_bucket_reso, bucket_reso_steps)\n",
        "    bucketManager.make_buckets()\n",
        "    for subset in subsets:\n",
        "        for image in Path(subset[\"image_dir\"]).iterdir():\n",
        "            if image.suffix not in supported_types:\n",
        "                continue\n",
        "            with Image.open(image) as img:\n",
        "                bucket_reso, _, _ = bucketManager.select_bucket(img.width, img.height)\n",
        "                for _ in range(subset[\"num_repeats\"]):\n",
        "                    bucketManager.add_image(bucket_reso, image)\n",
        "    steps_before_acc = sum(math.ceil(len(bucket) / train_batch_size) for bucket in bucketManager.buckets)\n",
        "    calculated_max_steps = math.ceil(steps_before_acc / gradient_accumulation_steps) * max_train_epochs\n",
        "    del bucketManager\n",
        "\n",
        "  cycle_steps = calculated_max_steps // (lr_scheduler_num_cycles or 1)\n",
        "  print(f\"  cycle steps: {cycle_steps}\")\n",
        "  lr_scheduler_args.append(f\"first_cycle_max_steps={cycle_steps}\")\n",
        "\n",
        "  warmup_steps = round(calculated_max_steps * lr_warmup_ratio) // (lr_scheduler_num_cycles or 1)\n",
        "  if warmup_steps > 0:\n",
        "    print(f\"  warmup steps: {warmup_steps}\")\n",
        "    lr_scheduler_args.append(f\"warmup_steps={warmup_steps}\")\n",
        "\n",
        "def main():\n",
        "  global dependencies_installed\n",
        "\n",
        "  if not os.path.exists('/content/drive'):\n",
        "    from google.colab import drive\n",
        "    print(\"📂 Conectando a Google Drive...\")\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "  for dir in (main_dir, trainer_dir, log_folder, images_folder, output_folder, config_folder):\n",
        "    os.makedirs(dir, exist_ok=True)\n",
        "\n",
        "  if not validate_dataset():\n",
        "    return\n",
        "\n",
        "  if not dependencies_installed:\n",
        "    print(\"\\n🏭 Instalando entrenador...\\n\")\n",
        "    t0 = time.time()\n",
        "    install_trainer()\n",
        "    t1 = time.time()\n",
        "    dependencies_installed = True\n",
        "    print(f\"\\n✅ Instalación terminada en {int(t1-t0)} segundos.\")\n",
        "  else:\n",
        "    print(\"\\n✅ Dependencias ya instaladas.\")\n",
        "\n",
        "  if old_model_url != model_url or not model_file or not os.path.exists(model_file):\n",
        "    print(\"\\n🔄 Obteniendo modelo...\")\n",
        "    if not download_model():\n",
        "      print(\"\\n💥 Error: el modelo que especificó no es válido o está corrupto.\"\n",
        "            \"\\nSi está utilizando una URL, verifique que el modelo sea accesible sin registrarse.\"\n",
        "            \"\\nPuede probar las URL Civitai o Huggingface, o una ruta en su Google Drive Comenzando con /content/drive/MyDrive\")\n",
        "      return\n",
        "    print()\n",
        "  else:\n",
        "    print(\"\\n🔄 Modelo ya descargado.\\n\")\n",
        "\n",
        "  if lr_scheduler_type:\n",
        "    create_config()\n",
        "    os.chdir(kohya_dir)\n",
        "    calculate_rex_steps()\n",
        "    os.chdir(root_dir)\n",
        "\n",
        "  create_config()\n",
        "\n",
        "  print(\"\\n⭐ Iniciando Entrenador..\\n\")\n",
        "\n",
        "  os.chdir(kohya_dir)\n",
        "  !{venv_python} {train_network} --config_file={config_file} --dataset_config={dataset_config_file}\n",
        "  os.chdir(root_dir)\n",
        "\n",
        "  if not get_ipython().__dict__.get('user_ns', {}).get('_exit_code', False):\n",
        "    display(Markdown(\"### ✅ ¡Hecho! [Ve a descargar tu Lora de Google Drive](https://drive.google.com/drive/my-drive)\\n\"\n",
        "                       \"### Habrá varios archivos, debe probar la última versión (el archivo con el número más grande junto a él)\"))\n",
        "\n",
        "main()\n",
        "#auto off\n",
        "if auto_off:\n",
        "    print(\"\\033[96mAuto Off encendido, el entorno se desconectara en 30 segundos\\033[0m\")\n",
        "    from google.colab import runtime\n",
        "    time.sleep(30)\n",
        "    runtime.unassign()\n",
        "else:\n",
        "    print(\"\\033[96mAuto off está desactivado, el entorno no se desconectará\\033[0m\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "23_YDYGhEnK0"
      },
      "outputs": [],
      "source": [
        "#@markdown ##Conectarse a Google Drive 📁 <p> Algunas de las opciones de abajo necesitan conexión a Google Drive\n",
        "#@markdown conectate aquí.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "print(\"Conexión establecida a Drive Exitosa!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "-cnM6xM_E6Jx"
      },
      "outputs": [],
      "source": [
        "#@markdown ### ↪️ Continuar\n",
        "\n",
        "#@markdown Aquí puede escribir una ruta en su Google Drive para cargar un archivo Lora existente y continuar entrenando. <P>\n",
        "#@markdown **Advertencia:** No es lo mismo que una larga sesión de entrenamiento. Los epochs comienzan desde cero, y puede obtener peores resultados.\n",
        "continue_from_lora = \"\" #@param {type:\"string\"}\n",
        "if continue_from_lora and not continue_from_lora.startswith(\"/content/drive/MyDrive\"):\n",
        "  import os\n",
        "  continue_from_lora = os.path.join(\"/content/drive/MyDrive\", continue_from_lora)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODpo0kcX3KRy"
      },
      "source": [
        "### 📚 Varias carpetas en el mismo conjunto de datos\n",
        "A continuación se muestra una plantilla que le permite definir varias carpetas en su conjunto de datos. Debe incluir la ubicación de cada carpeta y puede establecer un número diferente de repeticiones para cada una. Para agregar más carpetas, simplemente copie y pegue las secciones que comienzan con `[[datasets.subsets]]`.\n",
        "\n",
        "Al habilitar esto, se ignorará el número de repeticiones establecidas en la celda principal y también se ignorará la carpeta principal establecida por el nombre del proyecto.\n",
        "\n",
        "Puede convertir uno de ellos en una carpeta de regularización agregando `is_reg = true`  \n",
        "También puede establecer diferentes `keep_tokens`, `flip_aug`, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y037lagnJWmn"
      },
      "outputs": [],
      "source": [
        "custom_dataset = \"\"\"\n",
        "[[datasets]]\n",
        "\n",
        "[[datasets.subsets]]\n",
        "image_dir = \"/content/drive/MyDrive/Loras/example/dataset/good_images\"\n",
        "num_repeats = 3\n",
        "\n",
        "[[datasets.subsets]]\n",
        "image_dir = \"/content/drive/MyDrive/Loras/example/dataset/normal_images\"\n",
        "num_repeats = 1\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W84Jxf-U2TIU"
      },
      "outputs": [],
      "source": [
        "custom_dataset = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zci9GW_i831B"
      },
      "source": [
        "##Extras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ZIL7itnNaw5V"
      },
      "outputs": [],
      "source": [
        "#@markdown ## Colab Activo en el celular 📳\n",
        "#@markdown Ejecuta esta celda para mantener viva la pestaña en el celular (antes de ejecutar la celda de inicio) <p>\n",
        "#@markdown puedes convinarlo con esta [extensión](https://chromewebstore.google.com/detail/google-colab-keep-alive/bokldcdphgknojlbfhpbbgkggjfhhaek) en tu celular o pc (en celular necesitas un navegador con soporte a extensiones, `firefox` no funcionara)\n",
        "%%html\n",
        "<b>Pulsa play en el reproductor de música para mantener viva la pestaña antes de ejecutar la celda de inicio (sólo utiliza 13 MB de datos).</b><br/>\n",
        "<audio src=\"https://raw.githubusercontent.com/KoboldAI/KoboldAI-Client/main/colab/silence.m4a\" controls>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5ACrHePi3Pen"
      },
      "outputs": [],
      "source": [
        "#@markdown ### 🔢 Contar conjuntos de datos\n",
        "#@markdown Google Drive hace imposible contar los archivos en una carpeta, por lo que le mostrará el recuento de archivos en todas las carpetas y subcarpetas.\n",
        "folder = \"/content/drive/MyDrive/Loras/ejemplo/dataset\" #@param {type:\"string\"}\n",
        "\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "if not os.path.exists('/content/drive'):\n",
        "    print(\"📂 Connecting to Google Drive...\\n\")\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "tree = {}\n",
        "exclude = (\"_logs\", \"/output\")\n",
        "for i, (root, dirs, files) in enumerate(os.walk(folder, topdown=True)):\n",
        "  dirs[:] = [d for d in dirs if all(ex not in d for ex in exclude)]\n",
        "  images = len([f for f in files if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))])\n",
        "  captions = len([f for f in files if f.lower().endswith(\".txt\")])\n",
        "  others = len(files) - images - captions\n",
        "  path = root[folder.rfind(\"/\")+1:]\n",
        "  tree[path] = None if not images else f\"{images:>4} images | {captions:>4} captions |\"\n",
        "  if tree[path] and others:\n",
        "    tree[path] += f\" {others:>4} other files\"\n",
        "\n",
        "pad = max(len(k) for k in tree)\n",
        "print(\"\\n\".join(f\"📁{k.ljust(pad)} | {v}\" for k, v in tree.items() if v))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ImAtduziVp5h"
      },
      "outputs": [],
      "source": [
        "#@markdown ##Calculador de Repeticiones ⌛📝\n",
        "#@markdown Calcula el número de repeticiones a usar para entrenar tu lora, Recuerda que en `SDXL y Pony` se usa un batch de `4`.\n",
        "#@markdown Si usas colab pro calcula tus repeticiones con `8` de batch size\n",
        "# Define las Variables\n",
        "# Número de imágenes\n",
        "num_images = 40 # @param{type:\"number\"}\n",
        "# Número de repeticiones\n",
        "num_repeats = 4 # @param{type:\"number\"}\n",
        "# Número de epocas\n",
        "num_epochs = 10 # @param{type:\"number\"}\n",
        "# Tamaño de lote\n",
        "batch_size = 4 # @param{type:\"number\"}\n",
        "\n",
        "# Calcula el resultado\n",
        "resultado = (num_images * num_repeats * num_epochs) / batch_size\n",
        "\n",
        "# Muestra el resultado\n",
        "print(\"\\33[96mEl total de repeticiones es:\\033[0m\", resultado)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "1RfwB4UB3M_w"
      },
      "outputs": [],
      "source": [
        "#@markdown ### 📂 Descomprimir conjunto de datos\n",
        "#@markdown Es mucho más lento cargar archivos individuales en drive, por lo que es posible que desee cargar un archivo zip si tiene su conjunto de datos en su computadra, aqui puedes descomprimirlo.\n",
        "zip = \"/content/drive/MyDrive/my_dataset.zip\" #@param {type:\"string\"}\n",
        "extract_to = \"/content/drive/MyDrive/Loras/example/dataset\" #@param {type:\"string\"}\n",
        "\n",
        "import os, zipfile\n",
        "\n",
        "if not os.path.exists('/content/drive'):\n",
        "  from google.colab import drive\n",
        "  print(\"📂 Connecting to Google Drive...\")\n",
        "  drive.mount('/content/drive')\n",
        "\n",
        "os.makedirs(extract_to, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(zip, 'r') as f:\n",
        "  f.extractall(extract_to)\n",
        "\n",
        "print(\"✅ Done\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3lcQFnR7bwa"
      },
      "source": [
        "## Iniciar sesión y crear repositorio en huggingFace 🤗"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "8YlP0cNQyujy"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "from huggingface_hub import HfApi\n",
        "from huggingface_hub.utils import validate_repo_id, HfHubHTTPError\n",
        "\n",
        "# @markdown crea una cuenta de HuggingFace [aquí](https://huggingface.co/) o si ya la tienes simplemente usa tu token 👇\n",
        "# @markdown > Obten **tú** token `EN WRITE` de Huggingface [aquí](https://huggingface.co/settings/tokens) 👈\n",
        "write_token = \"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\"  # @param {type:\"string\"}\n",
        "#@markdown Complete esto si desea cargarlo en su organización, o simplemente déjelo vacío.\n",
        "orgs_name = \"\"  # @param{type:\"string\"}\n",
        "# @markdown Si su repositorio de modelo/conjunto de datos no existe, lo creará automáticamente.\n",
        "#@markdown No se permiten espacios usa `un guión bajo` para nombres largos\n",
        "\n",
        "#@markdown El `model_name` se usa para subir archivos individuales\n",
        "model_name = \"\"  # @param{type:\"string\"}\n",
        "#@markdown El dataset se usa para subir carpetas enteras.\n",
        "dataset_name = \"loras\"  # @param{type:\"string\"}\n",
        "#@markdown Marca esta casilla si quieres que tu repositorio/dataset sea privado de lo contrario sera publico\n",
        "make_private = True  # @param{type:\"boolean\"}\n",
        "\n",
        "def authenticate(write_token):\n",
        "    login(write_token, add_to_git_credential=True)\n",
        "    api = HfApi()\n",
        "    return api.whoami(write_token), api\n",
        "\n",
        "\n",
        "def create_repo(api, user, orgs_name, repo_name, repo_type, make_private=False):\n",
        "    global model_repo\n",
        "    global datasets_repo\n",
        "\n",
        "    if orgs_name == \"\":\n",
        "        repo_id = user[\"name\"] + \"/\" + repo_name.strip()\n",
        "    else:\n",
        "        repo_id = orgs_name + \"/\" + repo_name.strip()\n",
        "\n",
        "    try:\n",
        "        validate_repo_id(repo_id)\n",
        "        api.create_repo(repo_id=repo_id, repo_type=repo_type, private=make_private)\n",
        "        print(f\"{repo_type.capitalize()} repo '{repo_id}' didn't exist, creating repo\")\n",
        "    except HfHubHTTPError as e:\n",
        "        print(f\"{repo_type.capitalize()} repo '{repo_id}' exists, skipping create repo\")\n",
        "\n",
        "    if repo_type == \"model\":\n",
        "        model_repo = repo_id\n",
        "        print(f\"{repo_type.capitalize()} repo '{repo_id}' link: https://huggingface.co/{repo_id}\\n\")\n",
        "    else:\n",
        "        datasets_repo = repo_id\n",
        "        print(f\"{repo_type.capitalize()} repo '{repo_id}' link: https://huggingface.co/datasets/{repo_id}\\n\")\n",
        "\n",
        "user, api = authenticate(write_token)\n",
        "\n",
        "if model_name:\n",
        "    create_repo(api, user, orgs_name, model_name, \"model\", make_private)\n",
        "if dataset_name:\n",
        "    create_repo(api, user, orgs_name, dataset_name, \"dataset\", make_private)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFFNDwd87L_4"
      },
      "source": [
        "##Subir a huggingface_hub (Lora y dataset) 🤗"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "EVrt9JJRCxmk"
      },
      "outputs": [],
      "source": [
        "#@markdown ##subir lora (safetensors)\n",
        "from huggingface_hub import HfApi\n",
        "from pathlib import Path\n",
        "\n",
        "api = HfApi()\n",
        "file_path = \"/content/drive/MyDrive/Loras/Mi_proyecto/output/Mi_lora.safetensors\" #@param {type :\"string\"}\n",
        "#@markdown Comentario (Opcional) puede ser util para identificar versiones del lora, `ejemplo: pony, animagine, V1, V2 etc...`\n",
        "commit_message = \"\"  #@param {type :\"string\"}\n",
        "\n",
        "if file_path != \"\":\n",
        "  path_obj = Path(file_path)\n",
        "  trained_model = path_obj.parts[-1]\n",
        "\n",
        "  print(f\"Uploading {trained_model} to https://huggingface.co/\"+model_repo)\n",
        "  print(f\"Please wait...\")\n",
        "\n",
        "  api.upload_file(\n",
        "      path_or_fileobj=file_path,\n",
        "      path_in_repo=trained_model,\n",
        "      repo_id=model_repo,\n",
        "      commit_message=commit_message,\n",
        "  )\n",
        "\n",
        "  print(f\"Upload success, located at https://huggingface.co/\"+model_repo+\"/blob/main/\"+trained_model+\"\\n\")\n",
        "print('¡¡Subida finalizada!!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "68XMHMLe--If"
      },
      "outputs": [],
      "source": [
        "# @markdown ## Subir carpeta 📁 a HuggingFace 🤗\n",
        "from huggingface_hub import HfApi\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "api = HfApi()\n",
        "\n",
        "# @markdown Puedes subir una carpeta entera a Huggingface con tus loras entrenados, incluso puedes subir toda la carpeta de tu proyecto (incluido sub-carpetas), ten paciencia podria tardar un tiempo en cargar varios archivos.\n",
        "folder_path = \"/content/drive/MyDrive/Loras/Mi_lora/output\"  # @param {type :\"string\"}\n",
        "\n",
        "# @markdown Nombre personalizado para la carpeta a subir\n",
        "folder_name = \"Mi_lora\"  # @param {type :\"string\"}\n",
        "\n",
        "# @markdown Comentario (Opcional)\n",
        "commit_message = \"\"  # @param {type :\"string\"}\n",
        "\n",
        "if not commit_message:\n",
        "    commit_message = \"feat: upload folder\"\n",
        "\n",
        "def upload_folder(folder_path, folder_name):\n",
        "    print(f\"Uploading {folder_name} to https://huggingface.co/datasets/{datasets_repo}\")\n",
        "    print(\"Please wait...\")\n",
        "\n",
        "    api.upload_folder(\n",
        "        folder_path=folder_path,\n",
        "        path_in_repo=folder_name,\n",
        "        repo_id=datasets_repo,\n",
        "        repo_type=\"dataset\",\n",
        "        commit_message=commit_message,\n",
        "        ignore_patterns=\".ipynb_checkpoints\",\n",
        "    )\n",
        "    print(f\"Carga terminada, puedes encontrarlo en https://huggingface.co/datasets/{datasets_repo}/tree/main/{folder_name}\\n\")\n",
        "\n",
        "def upload():\n",
        "    if folder_path:\n",
        "        upload_folder(folder_path, folder_name)\n",
        "\n",
        "upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7CL4Ckp3SeI"
      },
      "source": [
        "# 📈 Gráficas del entrenamiento\n",
        "Puedes hacer esto después de ejecutar el entrenador.  No necesitas esto a menos que sepas lo que estás haciendo.  \n",
        " Es posible que la primera celda a continuación no cargue todos sus registros.  Continúe probando la segunda celda hasta que se hayan cargado todos los datos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_TRI3eX90Rp"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir={log_folder}/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rM5SLq990Rp"
      },
      "outputs": [],
      "source": [
        "from tensorboard import notebook\n",
        "notebook.display(port=6006, height=800)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
